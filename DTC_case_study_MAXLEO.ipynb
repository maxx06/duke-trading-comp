{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 160\n",
      "-rw-r--r--@ 1 max  staff  23050 Mar 30 01:09 DTC_case_study (1).ipynb\n",
      "-rw-r--r--@ 1 max  staff  56974 Mar 30 09:36 auction_prices.csv\n"
     ]
    }
   ],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date     TEC     MKT  DATAFC  DATAFO  TEC_VOLUME  MKT_VOLUME  \\\n",
      "0  2024-01-03  548.29  633.26   28.61   28.51    27197400    96582300   \n",
      "1  2024-01-04  546.47  633.68   28.75   28.33    23674700    90683400   \n",
      "2  2024-01-05  548.23  635.40   28.21   28.03    38008900   102026400   \n",
      "3  2024-01-08  540.13  626.95   28.24   28.21    48842300   149892000   \n",
      "4  2024-01-09  550.43  633.78   28.41   28.08    37178900   105016100   \n",
      "\n",
      "   DATAFC_VOLUME  DATAFO_VOLUME  MKT_IMBALANCE  TEC_IMBALANCE  \\\n",
      "0       89610300       90036218       -32638.0     -1653817.0   \n",
      "1       83296620       83692529      8961901.0       832427.0   \n",
      "2       85654260       86061375     -5676197.0     -1017302.0   \n",
      "3       96659244       97118665     -2958468.0      2150464.0   \n",
      "4       99204696       99676216      1565000.0      -708564.0   \n",
      "\n",
      "   DATAFO_IMBALANCE  DATAFC_IMBALANCE  \n",
      "0         -479551.0         -482876.0  \n",
      "1           72074.0        -1425029.0  \n",
      "2          291916.0          629758.0  \n",
      "3          524343.0        -1174935.0  \n",
      "4          620513.0         1852407.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "auction_prices = pd.read_csv('auction_prices.csv')\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "print(auction_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'TEC', 'MKT', 'DATAFC', 'DATAFO', 'TEC_VOLUME', 'MKT_VOLUME',\n",
      "       'DATAFC_VOLUME', 'DATAFO_VOLUME', 'MKT_IMBALANCE', 'TEC_IMBALANCE',\n",
      "       'DATAFO_IMBALANCE', 'DATAFC_IMBALANCE'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 503 entries, 0 to 502\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Date              503 non-null    object \n",
      " 1   TEC               503 non-null    float64\n",
      " 2   MKT               503 non-null    float64\n",
      " 3   DATAFC            503 non-null    float64\n",
      " 4   DATAFO            503 non-null    float64\n",
      " 5   TEC_VOLUME        503 non-null    int64  \n",
      " 6   MKT_VOLUME        503 non-null    int64  \n",
      " 7   DATAFC_VOLUME     503 non-null    int64  \n",
      " 8   DATAFO_VOLUME     503 non-null    int64  \n",
      " 9   MKT_IMBALANCE     503 non-null    float64\n",
      " 10  TEC_IMBALANCE     503 non-null    float64\n",
      " 11  DATAFO_IMBALANCE  503 non-null    float64\n",
      " 12  DATAFC_IMBALANCE  503 non-null    float64\n",
      "dtypes: float64(8), int64(4), object(1)\n",
      "memory usage: 51.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display column names\n",
    "print(auction_prices.columns)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(auction_prices.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.4)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/max/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/max/Library/Python/3.11/lib/python/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/max/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-macosx_11_0_arm64.whl (254 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pyparsing, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pandas-2.2.3 pyparsing-3.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an alternative reality, the era of continuous trading came to an abrupt end in the early 2010s in the aftermath of the Great Recession, replaced by a single daily auction that redefined global financial markets. The shift was not the product of a single event but rather the culmination of decades of discontent with the trajectory of modern finance. Public sentiment had turned against the financial industry, increasingly viewed as a system that prioritized speed and speculation over substance and societal contribution. Stories of brilliant minds dedicating their careers to shaving microseconds off trade execution times—rather than solving pressing global challenges—fueled a growing belief that finance had become a profound waste of human potential. This narrative gained traction in the public consciousness and became a rallying cry for reform.\n",
    "\n",
    "Behind the scenes, a quieter but equally consequential shift was taking place. Traditional stock exchanges, long the gatekeepers of market activity, found themselves losing ground to alternative trading venues like dark pools. Nearly half of all trading volume occurred in these opaque, off-exchange platforms, where institutional investors could transact large blocks of shares without moving the market. Frustrated by this erosion of their influence and revenue, exchanges began lobbying regulators for a radical restructuring of the market. Their argument was ostensibly about fairness and transparency, but critics noted that the proposed system of a single daily auction would concentrate trading volume—and fees—back into the exchanges’ hands. This alignment of public frustration and private lobbying created the conditions for sweeping change.\n",
    "\n",
    "The transition to the single auction system was implemented over a period of five years, during which market participants were forced to adapt to the new paradigm. Each day, orders could be submitted until a predetermined cutoff time, after which prices were set in a centralized auction. Exchanges marketed the system as a triumph of simplicity and fairness, claiming it would eliminate the chaos of continious trading and fragmentation. For the public, the promise of a less frenetic market resonated with a desire to see finance serve the broader economy rather than itself. However, many skeptics saw the reform as a power grab by exchanges, who now controlled the critical moment of price discovery.\n",
    "\n",
    "As the system took hold, its impact was profound. Liquidity during the rest of the day dried up; the absence of continuous trading reshaped the role of finance professionals, shifting emphasis from speed and execution to analysis and forecasting. While some hailed the changes as a return to fundamentals, others lamented the loss of opportunities for real-time price adjustments. Outside the financial sector, opinions were equally divided. Advocates pointed to the reduction in volatility and the restoration of trust in the markets, arguing that the auction system had lowered the cost of trading since there was no longer a bid-ask spread. Detractors, however, noted that the concentration of trading into a single moment increased systemic risks and sometimes resulted in large price swings. In hindsight, the move to single daily auctions was both a product of its time and a reflection of deeper societal shifts. It addressed many of the immediate concerns around market stability and transparency, yet it left lingering questions about the role of finance in the broader human endeavor. Whether it represented genuine progress or merely a consolidation of power remains a topic of debate among historians and economists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auction Reference Price** \n",
    "\n",
    "The auction reference price is the price at which the highest number of buy and sell orders can be matched during an auction, ensuring maximum trade execution. It serves as the clearing price where supply and demand are balanced and is also the price at which market participants can both buy and sell without incurring additional costs. This price is provided to you under the ticker in the auction dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auction Volume** \n",
    "\n",
    "Auction volume measures the total quantity of an asset traded during a specific auction. This data is reported to participants before the auction close, so **you see volume info for the upcoming auction before it finalizes**. This reflects the aggregate demand and supply participating in the auction at a given time. High auction volume typically indicates significant market interest and liquidity, as institutional and retail traders align their trades to the auction, aiming to achieve execution at a fair and representative price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auction Imbalance**\n",
    "\n",
    "Auction imbalance refers to the difference between the total buy and sell orders submitted for an asset in a scheduled auction, highlighting whether demand or supply dominates. The imbalance data is reported to participants before the auction close, so **you see imbalance info for the upcoming auction before it finalizes**. It serves as an indicator of market sentiment and helps market participants anticipate price movements. A positive imbalance suggests higher buying interest, potentially driving the auction price upward, while a negative imbalance indicates more selling pressure, likely pushing prices lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Market Composite Index (MKT)**\n",
    "\n",
    "The Market Composite Index (MKT) is a benchmark tracking the performance of a diversified portfolio of stocks across various sectors and market capitalizations. Representing a broad cross-section of the economy, this index includes companies from established industries such as healthcare and finance, as well as dynamic sectors like technology and energy. Designed to reflect the overall health of the equity markets, the MKT Index serves as a critical reference point for both individual investors and institutional funds.\n",
    "\n",
    "With moderate volatility, the MKT Index provides a balanced view of market trends, blending the stability of blue-chip stocks with the growth potential of emerging companies. Its performance is closely tied to macroeconomic indicators such as GDP, inflation, and interest rates, offering a snapshot of broader market sentiment. Ideal for long-term investors seeking a diversified market benchmark, the MKT Index remains a cornerstone of portfolio construction and performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tech Evolution Index (TEC)**\n",
    "\n",
    "The Tech Evolution Index (TEC) offers concentrated exposure to the high-growth technology sector, encompassing companies leading innovation in software, hardware, semiconductors, and digital services. This index is a barometer for the performance of technology-driven industries, tracking trends in artificial intelligence, cloud computing, and cybersecurity. TEC is widely regarded as an indicator of growth within the global economy's most dynamic sector.\n",
    "\n",
    "Known for its higher volatility compared to broader indices, the Tech Evolution Index attracts investors seeking to capitalize on technology’s rapid evolution and outsized economic contributions. The index is influenced by tech earnings reports, innovation cycles, and sector-specific developments like regulatory changes or global supply chain dynamics. For those looking to align their portfolios with cutting-edge innovation, TEC provides a focused lens on technology's market potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataForge Analytics Class A (DATAFO)**\n",
    "\n",
    "DataForge Analytics Class A (DATAFO) represents the voting shares of DataForge Analytics, a leader in enterprise data solutions, artificial intelligence, and cloud-based services. Designed for investors who value governance participation, DATAFO offers voting rights, allowing shareholders to influence corporate strategies and key decisions. This makes it an attractive option for institutional investors focused on long-term engagement.\n",
    "\n",
    "DATAFO reflects the company’s growth trajectory as it expands its footprint in enterprise analytics and digital transformation. Performance drivers include earnings results, customer acquisition, and industry partnerships, with external factors like advancements in AI and data privacy regulations adding complexity. With its strong market position and innovative product pipeline, DATAFO is ideal for investors seeking exposure to the data economy’s growing relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataForge Analytics Class C (DATAFC)**\n",
    "\n",
    "DataForge Analytics Class C (DATAFC) provides a non-voting alternative to DataForge's Class A shares, offering investors a cost-effective way to participate in the company’s financial success. While DATAFC lacks governance rights, it is designed for those prioritizing capital appreciation over corporate influence. As such, it sometimes trades at a slight discount to DATAFO but shares the same exposure to the company’s financial performance.\n",
    "\n",
    "Popular among active traders and passive investors alike, DATAFC combines liquidity with the opportunity to benefit from DataForge’s leadership in AI and data-driven solutions. Its performance mirrors that of DATAFO, driven by earnings growth, product innovation, and market trends in data analytics. For investors seeking affordable access to a tech-forward company poised for long-term growth, DATAFC is an excellent choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis and algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_imbalance_strategy(historical_df):\n",
    "    # Get the latest day's data\n",
    "    latest = historical_df.iloc[-1]\n",
    "   \n",
    "    # Calculate normalized imbalances\n",
    "    tec_norm_imb = latest['TEC_IMBALANCE'] / latest['TEC_VOLUME']\n",
    "    mkt_norm_imb = latest['MKT_IMBALANCE'] / latest['MKT_VOLUME']\n",
    "    datafc_norm_imb = latest['DATAFC_IMBALANCE'] / latest['DATAFC_VOLUME']\n",
    "    datafo_norm_imb = latest['DATAFO_IMBALANCE'] / latest['DATAFO_VOLUME']\n",
    "   \n",
    "    # Create list of tuples (asset, normalized imbalance)\n",
    "    assets = [\n",
    "        ('TEC', tec_norm_imb),\n",
    "        ('MKT', mkt_norm_imb),\n",
    "        ('DATAFC', datafc_norm_imb),\n",
    "        ('DATAFO', datafo_norm_imb)\n",
    "    ]\n",
    "   \n",
    "    # Sort by normalized imbalance (highest to lowest)\n",
    "    assets.sort(key=lambda x: x[1], reverse=True)\n",
    "   \n",
    "    # Initialize weights dictionary with default values\n",
    "    weights = {\n",
    "        'TEC': 0,\n",
    "        'MKT': 0,\n",
    "        'DATAFC': 0,\n",
    "        'DATAFO': 0,\n",
    "        'CASH': 0.15  # Default cash buffer\n",
    "    }\n",
    "   \n",
    "    # Dynamic cash adjustment based on volatility\n",
    "    if len(historical_df) >= 10:\n",
    "        # Calculate recent volatility\n",
    "        recent_returns = historical_df[['TEC', 'MKT', 'DATAFC', 'DATAFO']].pct_change().iloc[-10:].std()\n",
    "        avg_vol = recent_returns.mean()\n",
    "       \n",
    "        # Adjust cash based on volatility (between 10-30%)\n",
    "        weights['CASH'] = min(0.3, max(0.1, avg_vol * 10))\n",
    "   \n",
    "    # Assign weights based on ranking and signal strength\n",
    "    if len(assets) >= 1:\n",
    "        # Highest ranked asset\n",
    "        if abs(assets[0][1]) > 0.01:  # Very strong signal\n",
    "            weights[assets[0][0]] = 1.0\n",
    "            weights['CASH'] = 0.1  # Reduce cash to minimum\n",
    "        elif abs(assets[0][1]) > 0.005:  # Strong signal\n",
    "            weights[assets[0][0]] = 0.6\n",
    "        else:  # Moderate signal\n",
    "            weights[assets[0][0]] = 0.5\n",
    "   \n",
    "    if len(assets) >= 2:\n",
    "        # Second ranked asset\n",
    "        if abs(assets[1][1]) > 0.005:  # Strong signal\n",
    "            weights[assets[1][0]] = 0.3\n",
    "        else:  # Moderate signal\n",
    "            weights[assets[1][0]] = 0.25\n",
    "   \n",
    "    if len(assets) >= 3:\n",
    "        # Third ranked asset\n",
    "        weights[assets[2][0]] = 0.1\n",
    "   \n",
    "    if len(assets) >= 4:\n",
    "        # Lowest ranked asset (short position)\n",
    "        if assets[3][1] < -0.005:  # Strong negative signal\n",
    "            weights[assets[3][0]] = -0.2\n",
    "        else:  # Moderate negative signal\n",
    "            weights[assets[3][0]] = -0.1\n",
    "   \n",
    "    # Volume-based signal reinforcement\n",
    "    if len(historical_df) >= 2:\n",
    "        for asset_name, _ in assets:\n",
    "            volume_col = f\"{asset_name}_VOLUME\"\n",
    "            if volume_col in historical_df.columns:\n",
    "                volume_ratio = latest[volume_col] / historical_df[volume_col].iloc[-2]\n",
    "               \n",
    "                # Strengthen signals with volume confirmation\n",
    "                if volume_ratio > 1.2 and weights[asset_name] > 0:  # Volume spike, long position\n",
    "                    weights[asset_name] *= 1.2\n",
    "                elif volume_ratio > 1.2 and weights[asset_name] < 0:  # Volume spike, short position\n",
    "                    weights[asset_name] *= 1.3  # Strengthen short on volume confirmation\n",
    "   \n",
    "    # Ensure sum of weights equals 1.0\n",
    "    weight_sum = sum(weights.values())\n",
    "    if not np.isclose(weight_sum, 1.0):\n",
    "        # Adjust cash to make weights sum to 1.0\n",
    "        weights['CASH'] += (1.0 - weight_sum)\n",
    "   \n",
    "    # Validate leverage constraint (sum of abs values <= 3)\n",
    "    total_leverage = sum(abs(w) for w in weights.values())\n",
    "    if total_leverage > 3:\n",
    "        # Scale down proportionally\n",
    "        scale_factor = 3 / total_leverage\n",
    "        for asset in weights:\n",
    "            weights[asset] *= scale_factor\n",
    "       \n",
    "        # Ensure sum is still 1.0 after scaling\n",
    "        weights['CASH'] += (1.0 - sum(weights.values()))\n",
    "   \n",
    "    # Return weights in the required order\n",
    "    return np.array([\n",
    "        weights['TEC'],\n",
    "        weights['MKT'],\n",
    "        weights['DATAFC'],\n",
    "        weights['DATAFO'],\n",
    "        weights['CASH']\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_asymmetric_liquidity_strategy(historical_df):\n",
    "    # Handle edge case of first day\n",
    "    if len(historical_df) < 2:\n",
    "        return np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    \n",
    "    # Get the latest day's data\n",
    "    latest = historical_df.iloc[-1]\n",
    "    \n",
    "    # ----- ENHANCED SIGNAL PROCESSING -----\n",
    "    # 1. Calculate normalized imbalance ratios with volume scaling\n",
    "    # Use square root of volume to reduce impact of extreme volume days\n",
    "    imb_ratios = {\n",
    "        'TEC': latest['TEC_IMBALANCE'] / (latest['TEC_VOLUME'] ** 0.5),\n",
    "        'MKT': latest['MKT_IMBALANCE'] / (latest['MKT_VOLUME'] ** 0.5),\n",
    "        'DATAFC': latest['DATAFC_IMBALANCE'] / (latest['DATAFC_VOLUME'] ** 0.5),\n",
    "        'DATAFO': latest['DATAFO_IMBALANCE'] / (latest['DATAFO_VOLUME'] ** 0.5)\n",
    "    }\n",
    "    \n",
    "    # 2. Calculate z-scores if we have enough history (improves signal quality)\n",
    "    if len(historical_df) >= 10:\n",
    "        for asset in ['TEC', 'MKT', 'DATAFC', 'DATAFO']:\n",
    "            # Get historical normalized imbalances\n",
    "            hist_imb = []\n",
    "            for i in range(min(10, len(historical_df))):\n",
    "                day = historical_df.iloc[-(i+1)]\n",
    "                hist_imb.append(day[f'{asset}_IMBALANCE'] / (day[f'{asset}_VOLUME'] ** 0.5))\n",
    "            \n",
    "            # Calculate mean and std of historical values\n",
    "            mean_imb = np.mean(hist_imb)\n",
    "            std_imb = np.std(hist_imb) if np.std(hist_imb) > 0 else 1.0\n",
    "            \n",
    "            # Convert current ratio to z-score\n",
    "            imb_ratios[asset] = (imb_ratios[asset] - mean_imb) / std_imb\n",
    "    \n",
    "    # 3. Calculate relative cross-asset signal strength\n",
    "    # This helps identify assets with strongest relative imbalances\n",
    "    avg_imb_ratio = sum(imb_ratios.values()) / len(imb_ratios)\n",
    "    relative_imb = {asset: ratio - avg_imb_ratio for asset, ratio in imb_ratios.items()}\n",
    "    \n",
    "    # 4. Compute signal reliability score based on consistency\n",
    "    signal_reliability = {asset: 1.0 for asset in imb_ratios}  # Default value\n",
    "    if len(historical_df) >= 5:\n",
    "        for asset in imb_ratios:\n",
    "            # Check imbalance consistency (same direction) over past 3 days\n",
    "            consistent_count = 1  # Start with 1 for current day\n",
    "            current_sign = np.sign(latest[f'{asset}_IMBALANCE'])\n",
    "            for i in range(1, min(3, len(historical_df))):\n",
    "                prev_sign = np.sign(historical_df.iloc[-(i+1)][f'{asset}_IMBALANCE'])\n",
    "                if current_sign == prev_sign:\n",
    "                    consistent_count += 1\n",
    "            \n",
    "            # Higher reliability for consistent signals\n",
    "            signal_reliability[asset] = 0.8 + (0.2 * consistent_count / 3)\n",
    "    \n",
    "    # 5. Compute final combined signal\n",
    "    combined_signals = {}\n",
    "    for asset in imb_ratios:\n",
    "        # Weight absolute and relative signals, adjusted by reliability\n",
    "        combined_signals[asset] = ((0.6 * imb_ratios[asset]) + \n",
    "                                  (0.4 * relative_imb[asset])) * signal_reliability[asset]\n",
    "    \n",
    "    # Sort assets by combined signal (descending)\n",
    "    sorted_assets = sorted(combined_signals.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # ----- OPTIMIZED POSITION SIZING -----\n",
    "    # Initialize weights\n",
    "    weights = {'TEC': 0, 'MKT': 0, 'DATAFC': 0, 'DATAFO': 0, 'CASH': 0.15}\n",
    "    \n",
    "    # Calculate signal strength scores (0-1 range) for position sizing\n",
    "    max_signal = max(abs(s) for _, s in sorted_assets)\n",
    "    if max_signal > 0:\n",
    "        signal_strength = {asset: min(1.0, abs(signal)/max_signal) for asset, signal in sorted_assets}\n",
    "    else:\n",
    "        signal_strength = {asset: 0.5 for asset, _ in sorted_assets}  # Default if no signals\n",
    "    \n",
    "    # Position sizing based on rank and signal strength\n",
    "    # First asset - strongest signal\n",
    "    top_asset = sorted_assets[0][0]\n",
    "    top_signal = sorted_assets[0][1]\n",
    "    top_strength = signal_strength[top_asset]\n",
    "    \n",
    "    # Dynamic weight based on signal direction and strength\n",
    "    if top_signal > 0:\n",
    "        weights[top_asset] = 0.5 + (0.1 * top_strength)  # 0.5-0.6 range for longs\n",
    "    else:\n",
    "        weights[top_asset] = -0.3 * top_strength  # 0 to -0.3 range for shorts\n",
    "    \n",
    "    # Second asset\n",
    "    second_asset = sorted_assets[1][0]\n",
    "    second_signal = sorted_assets[1][1]\n",
    "    second_strength = signal_strength[second_asset]\n",
    "    \n",
    "    if second_signal > 0:\n",
    "        weights[second_asset] = 0.25 + (0.05 * second_strength)  # 0.25-0.3 range\n",
    "    else:\n",
    "        weights[second_asset] = -0.15 * second_strength  # 0 to -0.15 range\n",
    "    \n",
    "    # Third asset - only take meaningful position if signal is strong\n",
    "    third_asset = sorted_assets[2][0]\n",
    "    third_signal = sorted_assets[2][1]\n",
    "    third_strength = signal_strength[third_asset]\n",
    "    \n",
    "    if abs(third_signal) > 0.5:  # Only take position if signal is meaningful\n",
    "        if third_signal > 0:\n",
    "            weights[third_asset] = 0.1 + (0.05 * third_strength)  # 0.1-0.15 range\n",
    "        else:\n",
    "            weights[third_asset] = -0.05 * third_strength  # 0 to -0.05 range\n",
    "    \n",
    "    # Fourth asset - only short if very negative\n",
    "    fourth_asset = sorted_assets[3][0]\n",
    "    fourth_signal = sorted_assets[3][1]\n",
    "    if fourth_signal < -1.0:  # Only short if signal is strongly negative\n",
    "        weights[fourth_asset] = -0.05  # Small short position\n",
    "    \n",
    "    # Pair relationship between DATAFC and DATAFO (same company different share classes)\n",
    "    # Exploit relative value opportunities between the two share classes\n",
    "    if len(historical_df) >= 10:\n",
    "        # Calculate typical price ratio\n",
    "        ratio_series = historical_df['DATAFC'] / historical_df['DATAFO']\n",
    "        mean_ratio = ratio_series.iloc[-10:].mean()\n",
    "        current_ratio = latest['DATAFC'] / latest['DATAFO']\n",
    "        \n",
    "        # If DATAFC is cheap relative to DATAFO, shift some weight to DATAFC\n",
    "        if current_ratio < 0.98 * mean_ratio and weights['DATAFC'] >= 0:\n",
    "            pair_adjustment = min(0.05, weights['DATAFO'] * 0.2)\n",
    "            weights['DATAFC'] += pair_adjustment\n",
    "            weights['DATAFO'] -= pair_adjustment\n",
    "        \n",
    "        # If DATAFO is cheap relative to DATAFC, shift some weight to DATAFO\n",
    "        elif current_ratio > 1.02 * mean_ratio and weights['DATAFO'] >= 0:\n",
    "            pair_adjustment = min(0.05, weights['DATAFC'] * 0.2)\n",
    "            weights['DATAFC'] -= pair_adjustment\n",
    "            weights['DATAFO'] += pair_adjustment\n",
    "    \n",
    "    # ----- STRICT LEVERAGE CONTROL -----\n",
    "    # Calculate current leverage\n",
    "    leverage = sum(abs(w) for w in weights.values())\n",
    "    \n",
    "    # If approaching maximum, scale non-cash positions proportionally\n",
    "    if leverage > 2.7:  # Conservative buffer of 0.3\n",
    "        scaling_factor = 2.7 / leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= scaling_factor\n",
    "    \n",
    "    # Ensure weights sum to 1.0\n",
    "    non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "    weights['CASH'] = 1.0 - non_cash_sum\n",
    "    \n",
    "    # Final integrity check with numerical precision safety\n",
    "    final_leverage = sum(abs(w) for w in weights.values())\n",
    "    if final_leverage >= 2.99:  # If still too close to limit after adjustments\n",
    "        # Emergency fallback to conservative allocation\n",
    "        w_top = 0.5 if sorted_assets[0][1] > 0 else -0.2\n",
    "        w_second = 0.25 if sorted_assets[1][1] > 0 else -0.1\n",
    "        w_cash = 1.0 - (w_top + w_second)\n",
    "        \n",
    "        return np.array([\n",
    "            w_top if top_asset == 'TEC' else (w_second if second_asset == 'TEC' else 0),\n",
    "            w_top if top_asset == 'MKT' else (w_second if second_asset == 'MKT' else 0),\n",
    "            w_top if top_asset == 'DATAFC' else (w_second if second_asset == 'DATAFC' else 0),\n",
    "            w_top if top_asset == 'DATAFO' else (w_second if second_asset == 'DATAFO' else 0),\n",
    "            w_cash\n",
    "        ])\n",
    "    \n",
    "    return np.array([\n",
    "        weights['TEC'],\n",
    "        weights['MKT'],\n",
    "        weights['DATAFC'],\n",
    "        weights['DATAFO'],\n",
    "        weights['CASH']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_optimized_asymmetric_strategy(historical_df):\n",
    "    # Handle edge case of first day with neutral allocation\n",
    "    if len(historical_df) < 2:\n",
    "        return np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    \n",
    "    # Get the latest day's data\n",
    "    latest = historical_df.iloc[-1]\n",
    "    \n",
    "    # ----- REFINED SIGNAL PROCESSING -----\n",
    "    # 1. Calculate normalized imbalance with optimized volume scaling\n",
    "    # Cube root scaling provides better balance between large and small imbalances\n",
    "    imb_ratios = {\n",
    "        'TEC': latest['TEC_IMBALANCE'] / (latest['TEC_VOLUME'] ** (1/3)),\n",
    "        'MKT': latest['MKT_IMBALANCE'] / (latest['MKT_VOLUME'] ** (1/3)),\n",
    "        'DATAFC': latest['DATAFC_IMBALANCE'] / (latest['DATAFC_VOLUME'] ** (1/3)),\n",
    "        'DATAFO': latest['DATAFO_IMBALANCE'] / (latest['DATAFO_VOLUME'] ** (1/3))\n",
    "    }\n",
    "    \n",
    "    # 2. Improved Z-score calculation with exponential weighting\n",
    "    if len(historical_df) >= 10:\n",
    "        lookback = min(15, len(historical_df)-1)\n",
    "        for asset in ['TEC', 'MKT', 'DATAFC', 'DATAFO']:\n",
    "            # Calculate exponentially weighted historical values\n",
    "            hist_imb = []\n",
    "            weights = []\n",
    "            for i in range(lookback):\n",
    "                day = historical_df.iloc[-(i+1)]\n",
    "                hist_imb.append(day[f'{asset}_IMBALANCE'] / (day[f'{asset}_VOLUME'] ** (1/3)))\n",
    "                weights.append(0.85**i)  # Exponential decay factor\n",
    "            \n",
    "            # Calculate weighted mean and std\n",
    "            weighted_mean = np.average(hist_imb, weights=weights)\n",
    "            weighted_var = np.average([(x - weighted_mean)**2 for x in hist_imb], weights=weights)\n",
    "            weighted_std = np.sqrt(weighted_var) if weighted_var > 0 else 1.0\n",
    "            \n",
    "            # Convert to z-score with improved scaling\n",
    "            imb_ratios[asset] = (imb_ratios[asset] - weighted_mean) / weighted_std\n",
    "    \n",
    "    # 3. Cross-asset relative strength with sector adjustment\n",
    "    # Group TEC and MKT (indices) separately from DATAFC and DATAFO (same company)\n",
    "    idx_avg = (imb_ratios['TEC'] + imb_ratios['MKT']) / 2\n",
    "    stock_avg = (imb_ratios['DATAFC'] + imb_ratios['DATAFO']) / 2\n",
    "    \n",
    "    # Calculate relative strength within each group\n",
    "    relative_imb = {\n",
    "        'TEC': imb_ratios['TEC'] - idx_avg,\n",
    "        'MKT': imb_ratios['MKT'] - idx_avg,\n",
    "        'DATAFC': imb_ratios['DATAFC'] - stock_avg,\n",
    "        'DATAFO': imb_ratios['DATAFO'] - stock_avg\n",
    "    }\n",
    "    \n",
    "    # 4. Signal persistence and trend detection\n",
    "    signal_trend = {asset: 0 for asset in imb_ratios}\n",
    "    if len(historical_df) >= 4:\n",
    "        for asset in imb_ratios:\n",
    "            # Calculate recent trend in imbalance\n",
    "            imb_series = []\n",
    "            for i in range(min(4, len(historical_df))):\n",
    "                day = historical_df.iloc[-(i+1)]\n",
    "                imb_series.append(day[f'{asset}_IMBALANCE'] / day[f'{asset}_VOLUME'])\n",
    "            \n",
    "            # Trend strength (positive = strengthening imbalance)\n",
    "            differences = [imb_series[i] - imb_series[i+1] for i in range(len(imb_series)-1)]\n",
    "            signal_trend[asset] = np.mean(differences) * 10  # Scale factor\n",
    "    \n",
    "    # 5. Combine all signal components with optimized weights\n",
    "    combined_signals = {}\n",
    "    for asset in imb_ratios:\n",
    "        # Weight: 50% absolute signal, 30% relative signal, 20% trend\n",
    "        combined_signals[asset] = (0.5 * imb_ratios[asset] + \n",
    "                                  0.3 * relative_imb[asset] + \n",
    "                                  0.2 * signal_trend[asset])\n",
    "    \n",
    "    # Sort assets by combined signal (descending)\n",
    "    sorted_assets = sorted(combined_signals.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # ----- OPTIMAL POSITION SIZING -----\n",
    "    # Initialize weights with slightly reduced cash buffer given strategy strength\n",
    "    weights = {'TEC': 0, 'MKT': 0, 'DATAFC': 0, 'DATAFO': 0, 'CASH': 0.12}\n",
    "    \n",
    "    # Normalized signal strength for position sizing (0-1 range)\n",
    "    max_signal = max(abs(s) for _, s in sorted_assets)\n",
    "    if max_signal > 0:\n",
    "        signal_strength = {asset: min(1.0, abs(signal)/max_signal) for asset, signal in sorted_assets}\n",
    "    else:\n",
    "        signal_strength = {asset: 0.5 for asset, _ in sorted_assets}\n",
    "    \n",
    "    # Top asset - increased allocation given strong performance\n",
    "    top_asset = sorted_assets[0][0]\n",
    "    top_signal = sorted_assets[0][1]\n",
    "    top_strength = signal_strength[top_asset]\n",
    "    \n",
    "    if top_signal > 0:\n",
    "        weights[top_asset] = 0.55 + (0.15 * top_strength)  # 0.55-0.7 range\n",
    "    else:\n",
    "        weights[top_asset] = -0.35 * top_strength  # 0 to -0.35 range\n",
    "    \n",
    "    # Second asset - balanced allocation\n",
    "    second_asset = sorted_assets[1][0]\n",
    "    second_signal = sorted_assets[1][1]\n",
    "    second_strength = signal_strength[second_asset]\n",
    "    \n",
    "    if second_signal > 0:\n",
    "        weights[second_asset] = 0.25 + (0.15 * second_strength)  # 0.25-0.4 range\n",
    "    else:\n",
    "        weights[second_asset] = -0.2 * second_strength  # 0 to -0.2 range\n",
    "    \n",
    "    # Third asset - only meaningful position for strong signals\n",
    "    third_asset = sorted_assets[2][0]\n",
    "    third_signal = sorted_assets[2][1]\n",
    "    third_strength = signal_strength[third_asset]\n",
    "    \n",
    "    if abs(third_signal) > 0.4:  # Only take position if signal is meaningful\n",
    "        if third_signal > 0:\n",
    "            weights[third_asset] = 0.1 + (0.1 * third_strength)  # 0.1-0.2 range\n",
    "        else:\n",
    "            weights[third_asset] = -0.1 * third_strength  # 0 to -0.1 range\n",
    "    \n",
    "    # Fourth asset - only short if strongly negative\n",
    "    fourth_asset = sorted_assets[3][0]\n",
    "    fourth_signal = sorted_assets[3][1]\n",
    "    \n",
    "    if fourth_signal < -0.8:  # Only short if very negative\n",
    "        weights[fourth_asset] = -0.1 * signal_strength[fourth_asset]  # 0 to -0.1\n",
    "    \n",
    "    # ----- ENHANCED PAIR TRADING -----\n",
    "    # Improved DATAFC/DATAFO pair relationship management\n",
    "    if len(historical_df) >= 10:\n",
    "        # Calculate ratio and its recent behavior\n",
    "        ratio_series = historical_df['DATAFC'] / historical_df['DATAFO']\n",
    "        mean_ratio = ratio_series.iloc[-10:].mean()\n",
    "        std_ratio = ratio_series.iloc[-10:].std()\n",
    "        current_ratio = latest['DATAFC'] / latest['DATAFO']\n",
    "        \n",
    "        # Z-score of current ratio\n",
    "        ratio_zscore = (current_ratio - mean_ratio) / std_ratio if std_ratio > 0 else 0\n",
    "        \n",
    "        # Only adjust if deviation is significant\n",
    "        if abs(ratio_zscore) > 1.0:\n",
    "            # Size of adjustment based on z-score\n",
    "            pair_adjustment = min(0.1, 0.03 * abs(ratio_zscore))\n",
    "            \n",
    "            # Determine direction\n",
    "            if ratio_zscore < 0:  # DATAFC cheap relative to DATAFO\n",
    "                # Shift from DATAFO to DATAFC if we're long both or short both\n",
    "                if weights['DATAFC'] * weights['DATAFO'] > 0:  # Same sign\n",
    "                    adj = min(pair_adjustment, abs(weights['DATAFO']) * 0.4)\n",
    "                    weights['DATAFC'] += adj * np.sign(weights['DATAFC'])\n",
    "                    weights['DATAFO'] -= adj * np.sign(weights['DATAFO'])\n",
    "            else:  # DATAFO cheap relative to DATAFC\n",
    "                # Shift from DATAFC to DATAFO if we're long both or short both\n",
    "                if weights['DATAFC'] * weights['DATAFO'] > 0:  # Same sign\n",
    "                    adj = min(pair_adjustment, abs(weights['DATAFC']) * 0.4)\n",
    "                    weights['DATAFC'] -= adj * np.sign(weights['DATAFC'])\n",
    "                    weights['DATAFO'] += adj * np.sign(weights['DATAFO'])\n",
    "    \n",
    "    # ----- OPTIMAL LEVERAGE MANAGEMENT -----\n",
    "    # Calculate preliminary leverage\n",
    "    leverage = sum(abs(w) for w in weights.values())\n",
    "    \n",
    "    # Dynamic leverage targeting based on signal conviction\n",
    "    top_two_strength = (signal_strength[top_asset] + signal_strength[second_asset]) / 2\n",
    "    target_leverage = 2.5 + (0.4 * top_two_strength)  # 2.5 to 2.9 range\n",
    "    \n",
    "    # Adjust to target leverage while staying under limit\n",
    "    target_leverage = min(2.9, target_leverage)  # Never exceed 2.9\n",
    "    \n",
    "    if leverage > target_leverage:\n",
    "        # Scale down to target\n",
    "        scaling_factor = target_leverage / leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= scaling_factor\n",
    "    elif leverage < target_leverage * 0.9 and leverage > 1.5:\n",
    "        # Scale up to target if we're significantly under, but have meaningful positions\n",
    "        scaling_factor = target_leverage / leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= min(scaling_factor, 1.2)  # Limit scaling to 20%\n",
    "    \n",
    "    # Ensure weights sum to 1.0\n",
    "    non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "    weights['CASH'] = 1.0 - non_cash_sum\n",
    "    \n",
    "    # Final safety check\n",
    "    final_leverage = sum(abs(w) for w in weights.values())\n",
    "    if final_leverage > 2.98:  # If still too close to limit\n",
    "        # Make final adjustment to provide buffer\n",
    "        scaling_factor = 2.98 / final_leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= scaling_factor\n",
    "        \n",
    "        # Rebalance cash\n",
    "        non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "        weights['CASH'] = 1.0 - non_cash_sum\n",
    "    \n",
    "    return np.array([\n",
    "        weights['TEC'],\n",
    "        weights['MKT'],\n",
    "        weights['DATAFC'],\n",
    "        weights['DATAFO'],\n",
    "        weights['CASH']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_imbalance_divergence_strategy(historical_df):\n",
    "    \"\"\"\n",
    "    Optimized imbalance divergence strategy with enhanced signal detection,\n",
    "    adaptive position sizing, and precision risk management.\n",
    "    \n",
    "    Core improvements:\n",
    "    1. Adaptive thresholds for extreme imbalances\n",
    "    2. Multi-timeframe volume analysis\n",
    "    3. Weighted divergence detection incorporating momentum\n",
    "    4. Enhanced pair trading with dynamic sizing\n",
    "    5. Position conviction scoring system\n",
    "    \"\"\"\n",
    "    # Handle edge case of first day\n",
    "    if len(historical_df) < 2:\n",
    "        return np.array([0.25, 0.25, 0.25, 0.25, 0.0])\n",
    "    \n",
    "    # Get latest data\n",
    "    latest = historical_df.iloc[-1]\n",
    "    \n",
    "    # ----- 1. ENHANCED IMBALANCE EXTREMES DETECTION -----\n",
    "    # Calculate normalized imbalances\n",
    "    imb = {\n",
    "        'TEC': latest['TEC_IMBALANCE'] / latest['TEC_VOLUME'],\n",
    "        'MKT': latest['MKT_IMBALANCE'] / latest['MKT_VOLUME'],\n",
    "        'DATAFC': latest['DATAFC_IMBALANCE'] / latest['DATAFC_VOLUME'],\n",
    "        'DATAFO': latest['DATAFO_IMBALANCE'] / latest['DATAFO_VOLUME']\n",
    "    }\n",
    "    \n",
    "    # Z-score calculation with adaptive lookback\n",
    "    if len(historical_df) >= 10:\n",
    "        # Use longer lookback for indices, shorter for single stocks\n",
    "        lookback_idx = min(30, len(historical_df))\n",
    "        lookback_stock = min(15, len(historical_df))\n",
    "        \n",
    "        # Calculate z-scores with separate lookbacks\n",
    "        for asset in imb:\n",
    "            # Select appropriate lookback period\n",
    "            lookback = lookback_idx if asset in ['TEC', 'MKT'] else lookback_stock\n",
    "            \n",
    "            # Get historical imbalances with exponential weighting\n",
    "            hist_imb = []\n",
    "            weights = []\n",
    "            for i in range(1, lookback):\n",
    "                day = historical_df.iloc[-i]\n",
    "                hist_imb.append(day[f'{asset}_IMBALANCE'] / day[f'{asset}_VOLUME'])\n",
    "                weights.append(0.9**i)  # Exponential decay factor\n",
    "            \n",
    "            # Calculate weighted mean and std\n",
    "            mean_imb = np.average(hist_imb, weights=weights) if hist_imb else 0\n",
    "            weighted_var = np.average([(x - mean_imb)**2 for x in hist_imb], weights=weights) if hist_imb else 0.001\n",
    "            std_imb = np.sqrt(weighted_var) if weighted_var > 0 else 0.001\n",
    "            \n",
    "            # Convert to z-score\n",
    "            imb[asset] = (imb[asset] - mean_imb) / std_imb\n",
    "    \n",
    "    # ----- 2. MULTI-TIMEFRAME VOLUME ANALYSIS -----\n",
    "    volume_score = {}\n",
    "    if len(historical_df) >= 10:\n",
    "        for asset in ['TEC', 'MKT', 'DATAFC', 'DATAFO']:\n",
    "            # Short-term volume surge (1-day)\n",
    "            short_vol = historical_df[f'{asset}_VOLUME'].iloc[-2:].mean()\n",
    "            # Medium-term average volume (5-day)\n",
    "            medium_vol = historical_df[f'{asset}_VOLUME'].iloc[-6:-1].mean() if len(historical_df) >= 6 else short_vol\n",
    "            # Longer-term average volume (10-day)\n",
    "            long_vol = historical_df[f'{asset}_VOLUME'].iloc[-11:-1].mean() if len(historical_df) >= 11 else medium_vol\n",
    "            \n",
    "            # Calculate composite volume score (0-1 range)\n",
    "            st_surge = min(2.0, latest[f'{asset}_VOLUME'] / short_vol) - 1  # -1 to 1 range\n",
    "            mt_surge = min(2.0, latest[f'{asset}_VOLUME'] / medium_vol) - 1  # -1 to 1 range\n",
    "            lt_surge = min(2.0, latest[f'{asset}_VOLUME'] / long_vol) - 1  # -1 to 1 range\n",
    "            \n",
    "            # Weight short-term more heavily\n",
    "            volume_score[asset] = max(0, (0.5 * st_surge + 0.3 * mt_surge + 0.2 * lt_surge))\n",
    "    else:\n",
    "        # Default if not enough history\n",
    "        volume_score = {asset: 0.2 for asset in ['TEC', 'MKT', 'DATAFC', 'DATAFO']}\n",
    "    \n",
    "    # ----- 3. ENHANCED DIVERGENCE DETECTION -----\n",
    "    # Calculate weighted imbalance for indices and stocks\n",
    "    index_imb = (imb['TEC'] + imb['MKT']) / 2\n",
    "    stock_imb = (imb['DATAFC'] + imb['DATAFO']) / 2\n",
    "    \n",
    "    # Core divergence signal: indices and stocks moving in opposite directions\n",
    "    divergence_signal = index_imb * stock_imb < 0  # True if opposite signs\n",
    "    \n",
    "    # Add momentum component to divergence if enough history\n",
    "    divergence_strength = 1.0  # Default strength\n",
    "    if len(historical_df) >= 5:\n",
    "        # Calculate recent performance spread\n",
    "        idx_return = (historical_df['TEC'].iloc[-1] / historical_df['TEC'].iloc[-3] + \n",
    "                    historical_df['MKT'].iloc[-1] / historical_df['MKT'].iloc[-3]) / 2 - 1\n",
    "        \n",
    "        stock_return = (historical_df['DATAFC'].iloc[-1] / historical_df['DATAFC'].iloc[-3] + \n",
    "                      historical_df['DATAFO'].iloc[-1] / historical_df['DATAFO'].iloc[-3]) / 2 - 1\n",
    "        \n",
    "        # Stronger divergence if price action confirms imbalance divergence\n",
    "        if (idx_return > stock_return and index_imb > stock_imb) or (idx_return < stock_return and index_imb < stock_imb):\n",
    "            divergence_strength = 1.5\n",
    "    \n",
    "    # ----- 4. ENHANCED DATAFC/DATAFO PAIR TRADING -----\n",
    "    fc_fo_score = 0\n",
    "    fc_fo_direction = 0\n",
    "    \n",
    "    if len(historical_df) >= 10:\n",
    "        # Calculate current ratio and historical pattern\n",
    "        ratio_series = historical_df['DATAFC'] / historical_df['DATAFO']\n",
    "        current_ratio = latest['DATAFC'] / latest['DATAFO']\n",
    "        \n",
    "        # Use exponentially weighted mean and std for better responsiveness\n",
    "        weights = [0.9**i for i in range(min(20, len(ratio_series)-1))]\n",
    "        ratio_hist = ratio_series.iloc[-(len(weights)+1):-1]\n",
    "        \n",
    "        mean_ratio = np.average(ratio_hist, weights=weights) if len(ratio_hist) > 0 else current_ratio\n",
    "        \n",
    "        # Calculate weighted std\n",
    "        weighted_var = np.average([(r - mean_ratio)**2 for r in ratio_hist], weights=weights) if len(ratio_hist) > 0 else 0.001\n",
    "        std_ratio = np.sqrt(weighted_var) if weighted_var > 0 else 0.01\n",
    "        \n",
    "        # Z-score of current ratio\n",
    "        ratio_zscore = (current_ratio - mean_ratio) / std_ratio\n",
    "        \n",
    "        # Score based on z-score magnitude (0-1 range)\n",
    "        fc_fo_score = min(1.0, abs(ratio_zscore) / 2.0)\n",
    "        \n",
    "        # Direction: positive = buy DATAFC, sell DATAFO; negative = opposite\n",
    "        fc_fo_direction = -np.sign(ratio_zscore)\n",
    "    \n",
    "    # ----- 5. POSITION CONVICTION SCORING -----\n",
    "    # Calculate conviction scores for each asset (0-1 range)\n",
    "    conviction = {}\n",
    "    \n",
    "    for asset in ['TEC', 'MKT', 'DATAFC', 'DATAFO']:\n",
    "        # Base score on imbalance strength\n",
    "        imb_score = min(1.0, abs(imb[asset]) / 2.0)\n",
    "        \n",
    "        # Combine with volume score\n",
    "        conviction[asset] = 0.7 * imb_score + 0.3 * volume_score[asset]\n",
    "    \n",
    "    # ----- 6. ADAPTIVE PORTFOLIO CONSTRUCTION -----\n",
    "    # Initialize weights with dynamic cash buffer\n",
    "    base_cash = 0.15  # Base cash allocation\n",
    "    # Reduce cash when signals are strong\n",
    "    avg_conviction = sum(conviction.values()) / len(conviction)\n",
    "    cash_buffer = max(0.10, base_cash - (0.10 * avg_conviction))\n",
    "    \n",
    "    weights = {'TEC': 0, 'MKT': 0, 'DATAFC': 0, 'DATAFO': 0, 'CASH': cash_buffer}\n",
    "    \n",
    "    # --- Index allocation based on optimized thresholds ---\n",
    "    # Dynamic threshold based on historical volatility\n",
    "    idx_threshold = 1.3 if len(historical_df) < 20 else 1.5\n",
    "    \n",
    "    # TEC position\n",
    "    if abs(imb['TEC']) > idx_threshold:  # Strong signal\n",
    "        position_size = 0.3 + (0.2 * conviction['TEC'])  # 0.3-0.5 range\n",
    "        weights['TEC'] = position_size * np.sign(imb['TEC'])\n",
    "    \n",
    "    # MKT position\n",
    "    if abs(imb['MKT']) > idx_threshold:  # Strong signal\n",
    "        position_size = 0.3 + (0.2 * conviction['MKT'])  # 0.3-0.5 range\n",
    "        weights['MKT'] = position_size * np.sign(imb['MKT'])\n",
    "    \n",
    "    # --- Exploit divergence with improved sizing ---\n",
    "    if divergence_signal:\n",
    "        # Focus on the index signal\n",
    "        if index_imb > 0:  # Positive index imbalance\n",
    "            # Increase long index positions\n",
    "            if weights['TEC'] >= 0:\n",
    "                weights['TEC'] = max(weights['TEC'], 0.3 * divergence_strength)\n",
    "            if weights['MKT'] >= 0:\n",
    "                weights['MKT'] = max(weights['MKT'], 0.3 * divergence_strength)\n",
    "                \n",
    "            # Reduce or short stock positions\n",
    "            if stock_imb < -idx_threshold:  # Only if stock signal is strong\n",
    "                weights['DATAFC'] = min(weights.get('DATAFC', 0), -0.15 * divergence_strength)\n",
    "                weights['DATAFO'] = min(weights.get('DATAFO', 0), -0.15 * divergence_strength)\n",
    "        else:  # Negative index imbalance\n",
    "            # Increase short index positions\n",
    "            if weights['TEC'] <= 0:\n",
    "                weights['TEC'] = min(weights['TEC'], -0.3 * divergence_strength)\n",
    "            if weights['MKT'] <= 0:\n",
    "                weights['MKT'] = min(weights['MKT'], -0.3 * divergence_strength)\n",
    "                \n",
    "            # Increase long stock positions\n",
    "            if stock_imb > idx_threshold:  # Only if stock signal is strong\n",
    "                weights['DATAFC'] = max(weights.get('DATAFC', 0), 0.15 * divergence_strength)\n",
    "                weights['DATAFO'] = max(weights.get('DATAFO', 0), 0.15 * divergence_strength)\n",
    "    \n",
    "    # --- Enhanced DATAFC/DATAFO pair trading ---\n",
    "    if fc_fo_score > 0.5:  # Only trade significant deviations\n",
    "        # Size based on z-score magnitude\n",
    "        pair_size = 0.15 + (0.15 * fc_fo_score)  # 0.15-0.3 range\n",
    "        \n",
    "        # Apply pair trade with dynamic sizing\n",
    "        if fc_fo_direction > 0:  # Buy DATAFC, sell DATAFO\n",
    "            weights['DATAFC'] += pair_size\n",
    "            weights['DATAFO'] -= pair_size\n",
    "        else:  # Sell DATAFC, buy DATAFO\n",
    "            weights['DATAFC'] -= pair_size\n",
    "            weights['DATAFO'] += pair_size\n",
    "    else:\n",
    "        # When no pair opportunity, use imbalance signals for stocks\n",
    "        stock_threshold = 1.2 if len(historical_df) < 20 else 1.4\n",
    "        \n",
    "        if abs(imb['DATAFC']) > stock_threshold:\n",
    "            position_size = 0.25 + (0.15 * conviction['DATAFC'])  # 0.25-0.4 range\n",
    "            weights['DATAFC'] = position_size * np.sign(imb['DATAFC'])\n",
    "            \n",
    "        if abs(imb['DATAFO']) > stock_threshold:\n",
    "            position_size = 0.25 + (0.15 * conviction['DATAFO'])  # 0.25-0.4 range\n",
    "            weights['DATAFO'] = position_size * np.sign(imb['DATAFO'])\n",
    "    \n",
    "    # ----- 7. PRECISION RISK MANAGEMENT -----\n",
    "    # Calculate current leverage\n",
    "    leverage = sum(abs(w) for w in weights.values())\n",
    "    \n",
    "    # Target leverage based on average conviction\n",
    "    target_leverage = 2.5 + (0.4 * avg_conviction)  # 2.5-2.9 range\n",
    "    target_leverage = min(2.85, target_leverage)  # Safety cap\n",
    "    \n",
    "    # Adjust to target leverage\n",
    "    if leverage > target_leverage:\n",
    "        scale_factor = target_leverage / leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= scale_factor\n",
    "    \n",
    "    # Ensure weights sum to 1.0\n",
    "    non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "    weights['CASH'] = 1.0 - non_cash_sum\n",
    "    \n",
    "    # Final leverage safety check with numerical precision buffer\n",
    "    final_leverage = sum(abs(w) for w in weights.values())\n",
    "    if final_leverage > 2.97:\n",
    "        scale_factor = 2.97 / final_leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= scale_factor\n",
    "        \n",
    "        # Readjust cash\n",
    "        non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "        weights['CASH'] = 1.0 - non_cash_sum\n",
    "    \n",
    "    return np.array([\n",
    "        weights['TEC'],\n",
    "        weights['MKT'],\n",
    "        weights['DATAFC'],\n",
    "        weights['DATAFO'],\n",
    "        weights['CASH']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_ensemble_strategy(historical_df):\n",
    "    # ----- ENSEMBLE LOGIC -----\n",
    "    # Run all three strategies independently\n",
    "    hyper_weights = hyper_optimized_asymmetric_strategy(historical_df)\n",
    "    imbalance_weights = optimized_imbalance_strategy(historical_df)\n",
    "    pair_weights = optimized_imbalance_divergence_strategy(historical_df)\n",
    "    \n",
    "    # Default weights for each strategy\n",
    "    strategy_weights = {\n",
    "        'hyper': 0.4,        # Sophisticated and highest-performing\n",
    "        'imbalance': 0.3,    # Simple and reliable\n",
    "        'pair': 0.3          # Different focus (pair trading)\n",
    "    }\n",
    "    \n",
    "    # Dynamic weighting based on recent performance (if enough history)\n",
    "    if len(historical_df) >= 10:\n",
    "        # Initialize performance metrics\n",
    "        strategy_returns = {'hyper': 0, 'imbalance': 0, 'pair': 0}\n",
    "        strategy_vol = {'hyper': 0, 'imbalance': 0, 'pair': 0}\n",
    "        \n",
    "        # Evaluate 10-day lookback performance\n",
    "        for i in range(1, min(10, len(historical_df))):\n",
    "            day_minus_1 = historical_df.iloc[-i-1]\n",
    "            day = historical_df.iloc[-i]\n",
    "            \n",
    "            # Get price changes\n",
    "            r_tec = day['TEC'] / day_minus_1['TEC'] - 1\n",
    "            r_mkt = day['MKT'] / day_minus_1['MKT'] - 1\n",
    "            r_datafc = day['DATAFC'] / day_minus_1['DATAFC'] - 1\n",
    "            r_datafo = day['DATAFO'] / day_minus_1['DATAFO'] - 1\n",
    "            r_cash = 0.045 / 252  # Risk-free rate daily\n",
    "            \n",
    "            # Run strategies for day_minus_1\n",
    "            hyper_w = hyper_optimized_asymmetric_strategy(historical_df.iloc[:-i])\n",
    "            imbalance_w = optimized_imbalance_strategy(historical_df.iloc[:-i])\n",
    "            pair_w = optimized_imbalance_divergence_strategy(historical_df.iloc[:-i])\n",
    "            \n",
    "            # Calculate daily returns for each strategy\n",
    "            returns = {}\n",
    "            for name, w in [('hyper', hyper_w), ('imbalance', imbalance_w), ('pair', pair_w)]:\n",
    "                day_return = (\n",
    "                    w[0] * r_tec +\n",
    "                    w[1] * r_mkt +\n",
    "                    w[2] * r_datafc +\n",
    "                    w[3] * r_datafo +\n",
    "                    w[4] * r_cash\n",
    "                )\n",
    "                strategy_returns[name] += day_return\n",
    "                strategy_vol[name] += day_return**2\n",
    "        \n",
    "        # Calculate risk-adjusted performance metrics\n",
    "        sharpe_ratios = {}\n",
    "        for name in strategy_returns:\n",
    "            if strategy_vol[name] > 0:\n",
    "                sharpe_ratios[name] = strategy_returns[name] / np.sqrt(strategy_vol[name])\n",
    "            else:\n",
    "                sharpe_ratios[name] = 0\n",
    "        \n",
    "        # Dynamic weighting based on Sharpe ratios\n",
    "        total_sharpe = sum(abs(sr) for sr in sharpe_ratios.values())\n",
    "        if total_sharpe > 0:\n",
    "            # Assign weights proportional to Sharpe ratios (with minimum weights)\n",
    "            for name in strategy_weights:\n",
    "                # Minimum 20% weight per strategy, rest distributed by performance\n",
    "                strategy_weights[name] = 0.2 + (0.4 * abs(sharpe_ratios[name]) / total_sharpe)\n",
    "            \n",
    "            # Normalize to ensure sum = 1.0\n",
    "            weight_sum = sum(strategy_weights.values())\n",
    "            for name in strategy_weights:\n",
    "                strategy_weights[name] /= weight_sum\n",
    "    \n",
    "    # Combine weights according to strategy weights\n",
    "    ensemble_weights = (\n",
    "        strategy_weights['hyper'] * hyper_weights +\n",
    "        strategy_weights['imbalance'] * imbalance_weights +\n",
    "        strategy_weights['pair'] * pair_weights\n",
    "    )\n",
    "    \n",
    "    # Ensure ensemble weights meet all constraints\n",
    "    # 1. Make sure sum is exactly 1.0\n",
    "    if not np.isclose(np.sum(ensemble_weights), 1.0):\n",
    "        # Adjust cash to make sum 1.0\n",
    "        non_cash_sum = np.sum(ensemble_weights[:4])\n",
    "        ensemble_weights[4] = 1.0 - non_cash_sum\n",
    "    \n",
    "    # 2. Make sure leverage constraint is satisfied\n",
    "    ensemble_leverage = np.sum(np.abs(ensemble_weights))\n",
    "    \n",
    "    if ensemble_leverage > 2.98:\n",
    "        # Scale down non-cash components\n",
    "        scale_factor = 2.98 / ensemble_leverage\n",
    "        \n",
    "        # Apply scaling to non-cash components\n",
    "        for i in range(4):  # First 4 elements are non-cash\n",
    "            ensemble_weights[i] *= scale_factor\n",
    "        \n",
    "        # Re-adjust cash to maintain sum = 1.0\n",
    "        non_cash_sum = np.sum(ensemble_weights[:4])\n",
    "        ensemble_weights[4] = 1.0 - non_cash_sum\n",
    "    \n",
    "    return ensemble_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_ensemble_strategy(historical_df):\n",
    "    \"\"\"\n",
    "    Optimized ensemble strategy that heavily weights data-driven approaches\n",
    "    based on the correlation analysis, particularly emphasizing DATAFC signals.\n",
    "    \"\"\"\n",
    "    # Handle edge case of first day\n",
    "    if len(historical_df) < 2:\n",
    "        return np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    \n",
    "    # Get latest data\n",
    "    latest = historical_df.iloc[-1]\n",
    "    \n",
    "    # ----- FEATURE ENGINEERING -----\n",
    "    # 1. Create normalized imbalance features\n",
    "    norm_imb = {\n",
    "        'TEC': latest['TEC_IMBALANCE'] / latest['TEC_VOLUME'],\n",
    "        'MKT': latest['MKT_IMBALANCE'] / latest['MKT_VOLUME'],\n",
    "        'DATAFC': latest['DATAFC_IMBALANCE'] / latest['DATAFC_VOLUME'],\n",
    "        'DATAFO': latest['DATAFO_IMBALANCE'] / latest['DATAFO_VOLUME']\n",
    "    }\n",
    "    \n",
    "    # 2. Calculate DATAFC/DATAFO ratio for pair trading\n",
    "    fc_fo_ratio = latest['DATAFC'] / latest['DATAFO']\n",
    "    \n",
    "    # 3. Calculate z-scores with exponential weighting\n",
    "    z_scores = dict(norm_imb)\n",
    "    if len(historical_df) >= 10:\n",
    "        for asset in norm_imb:\n",
    "            # Get historical normalized imbalances with exponential weighting\n",
    "            hist_values = []\n",
    "            weights = []\n",
    "            for i in range(1, min(30, len(historical_df))):\n",
    "                day = historical_df.iloc[-i]\n",
    "                hist_values.append(day[f'{asset}_IMBALANCE'] / day[f'{asset}_VOLUME'])\n",
    "                weights.append(0.95**i)  # Exponential decay factor\n",
    "            \n",
    "            # Calculate weighted mean and std\n",
    "            mean_val = np.average(hist_values, weights=weights) if hist_values else 0\n",
    "            weighted_var = np.average([(x - mean_val)**2 for x in hist_values], weights=weights) if hist_values else 0.001\n",
    "            std_val = np.sqrt(weighted_var) if weighted_var > 0 else 0.001\n",
    "            \n",
    "            # Convert to z-score\n",
    "            z_scores[asset] = (norm_imb[asset] - mean_val) / std_val\n",
    "    \n",
    "    # ----- STRATEGY 1: ENHANCED DATAFC-FOCUSED STRATEGY -----\n",
    "    def datafc_focused_strategy():\n",
    "        \"\"\"\n",
    "        Strategy heavily emphasizing DATAFC signals given its 0.4673 correlation,\n",
    "        which is ~3x stronger than other assets\n",
    "        \"\"\"\n",
    "        weights = {'TEC': 0, 'MKT': 0, 'DATAFC': 0, 'DATAFO': 0, 'CASH': 0.15}\n",
    "        \n",
    "        # DATAFC position - much higher weight given its strong predictive power\n",
    "        datafc_signal = z_scores['DATAFC']\n",
    "        if abs(datafc_signal) > 0.8:  # Lower threshold due to strong signal\n",
    "            # Strong position when signal exceeds threshold\n",
    "            position_size = 0.6 + (0.2 * min(1.0, abs(datafc_signal) / 2.0))  # 0.6-0.8 range\n",
    "            weights['DATAFC'] = position_size * np.sign(datafc_signal)\n",
    "        \n",
    "        # Other assets - much lower weights\n",
    "        other_assets = [('TEC', 0.1623), ('MKT', 0.1566), ('DATAFO', 0.1279)]\n",
    "        for asset, corr in other_assets:\n",
    "            if abs(z_scores[asset]) > 1.2:  # Higher threshold for weaker signals\n",
    "                # Scale by correlation strength\n",
    "                max_position = 0.15 + (0.1 * corr)  # 0.15-0.25 range\n",
    "                position_size = max_position * min(1.0, abs(z_scores[asset]) / 2.0)\n",
    "                weights[asset] = position_size * np.sign(z_scores[asset])\n",
    "        \n",
    "        # Ensure weights sum to 1.0\n",
    "        non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "        weights['CASH'] = 1.0 - non_cash_sum\n",
    "        \n",
    "        return np.array([\n",
    "            weights['TEC'],\n",
    "            weights['MKT'],\n",
    "            weights['DATAFC'],\n",
    "            weights['DATAFO'],\n",
    "            weights['CASH']\n",
    "        ])\n",
    "    \n",
    "    # ----- STRATEGY 2: CORRELATION-WEIGHTED STRATEGY -----\n",
    "    def correlation_weighted_strategy():\n",
    "        \"\"\"\n",
    "        Strategy with positions directly proportional to\n",
    "        asset correlations with next-day returns\n",
    "        \"\"\"\n",
    "        weights = {'TEC': 0, 'MKT': 0, 'DATAFC': 0, 'DATAFO': 0, 'CASH': 0.15}\n",
    "        \n",
    "        # Define correlation coefficients from data analysis\n",
    "        correlations = {\n",
    "            'DATAFC': 0.4673,\n",
    "            'TEC': 0.1623,\n",
    "            'MKT': 0.1566,\n",
    "            'DATAFO': 0.1279\n",
    "        }\n",
    "        \n",
    "        # Normalize correlations to sum to 1\n",
    "        total_corr = sum(correlations.values())\n",
    "        norm_corr = {k: v/total_corr for k, v in correlations.items()}\n",
    "        \n",
    "        # Allocate based on signal strength and normalized correlation\n",
    "        for asset, corr in norm_corr.items():\n",
    "            if abs(z_scores[asset]) > 1.0:\n",
    "                # Base position on correlation strength\n",
    "                position_size = 0.8 * corr * (1 + min(1.0, abs(z_scores[asset]) / 2.0))\n",
    "                weights[asset] = position_size * np.sign(z_scores[asset])\n",
    "        \n",
    "        # Ensure weights sum to 1.0\n",
    "        non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "        weights['CASH'] = 1.0 - non_cash_sum\n",
    "        \n",
    "        return np.array([\n",
    "            weights['TEC'],\n",
    "            weights['MKT'],\n",
    "            weights['DATAFC'],\n",
    "            weights['DATAFO'],\n",
    "            weights['CASH']\n",
    "        ])\n",
    "    \n",
    "    # ----- STRATEGY 3: PAIR TRADING STRATEGY (UPDATED) -----\n",
    "    def enhanced_pair_trading_strategy():\n",
    "        \"\"\"\n",
    "        Enhanced pair trading with asymmetric weightings based on\n",
    "        asset correlations\n",
    "        \"\"\"\n",
    "        weights = {'TEC': 0, 'MKT': 0, 'DATAFC': 0, 'DATAFO': 0, 'CASH': 0.25}\n",
    "        \n",
    "        # Calculate historical ratio statistics with exponential weighting\n",
    "        if len(historical_df) >= 10:\n",
    "            ratio_series = historical_df['DATAFC'] / historical_df['DATAFO']\n",
    "            \n",
    "            # Exponential weighting\n",
    "            weights_exp = [0.95**i for i in range(min(30, len(ratio_series)-1))]\n",
    "            ratio_hist = ratio_series.iloc[-(len(weights_exp)+1):-1]\n",
    "            \n",
    "            # Calculate weighted statistics\n",
    "            mean_ratio = np.average(ratio_hist, weights=weights_exp) if len(ratio_hist) > 0 else fc_fo_ratio\n",
    "            \n",
    "            # Calculate weighted std\n",
    "            weighted_var = np.average([(r - mean_ratio)**2 for r in ratio_hist], weights=weights_exp) if len(ratio_hist) > 0 else 0.001\n",
    "            std_ratio = np.sqrt(weighted_var) if weighted_var > 0 else 0.01\n",
    "            \n",
    "            # Z-score of current ratio\n",
    "            ratio_zscore = (fc_fo_ratio - mean_ratio) / std_ratio\n",
    "            \n",
    "            # Asymmetric pair trade based on correlations\n",
    "            if abs(ratio_zscore) > 1.0:\n",
    "                # Base size on deviation magnitude\n",
    "                size = 0.3 + (0.3 * min(1.0, abs(ratio_zscore) / 2.0))  # 0.3-0.6 range\n",
    "                \n",
    "                # DATAFC has stronger predictive power, so favor buying it\n",
    "                if ratio_zscore < 0:  # DATAFC cheap relative to DATAFO\n",
    "                    # Stronger position in DATAFC (higher correlation)\n",
    "                    weights['DATAFC'] = size * 1.2  # 20% stronger\n",
    "                    weights['DATAFO'] = -size\n",
    "                else:  # DATAFO cheap relative to DATAFC\n",
    "                    weights['DATAFC'] = -size\n",
    "                    weights['DATAFO'] = size * 0.8  # 20% weaker\n",
    "        \n",
    "        # Add index positions based on correlations and imbalance\n",
    "        for asset, corr in [('TEC', 0.1623), ('MKT', 0.1566)]:\n",
    "            if abs(z_scores[asset]) > 1.3:  # Only take strong signals\n",
    "                position_size = 0.2 * corr/0.16 * np.sign(z_scores[asset])  # Scale by correlation\n",
    "                weights[asset] = position_size\n",
    "        \n",
    "        # Ensure weights sum to 1.0\n",
    "        non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "        weights['CASH'] = 1.0 - non_cash_sum\n",
    "        \n",
    "        return np.array([\n",
    "            weights['TEC'],\n",
    "            weights['MKT'],\n",
    "            weights['DATAFC'],\n",
    "            weights['DATAFO'],\n",
    "            weights['CASH']\n",
    "        ])\n",
    "    \n",
    "    # ----- ENSEMBLE LOGIC -----\n",
    "    # Get allocations from all strategies\n",
    "    datafc_weights = datafc_focused_strategy()\n",
    "    corr_weights = correlation_weighted_strategy()\n",
    "    pair_weights = enhanced_pair_trading_strategy()\n",
    "    hyper_weights = hyper_optimized_asymmetric_strategy(historical_df)\n",
    "    imbalance_weights = optimized_imbalance_strategy(historical_df)\n",
    "    divergence_weights = optimized_imbalance_divergence_strategy(historical_df)\n",
    "    \n",
    "    # Define optimized strategy weights based on data analysis\n",
    "    # Heavily favor strategies that directly use correlation data\n",
    "    strategy_weights = {\n",
    "        'datafc': 0.30,      # Heavy emphasis on DATAFC signal\n",
    "        'corr': 0.25,        # Direct use of correlation coefficients\n",
    "        'pair': 0.15,        # Enhanced pair trading with asymmetric weights\n",
    "        'hyper': 0.10,       # Reduced weight for non-data-analysis strategies\n",
    "        'imbalance': 0.10,   # Reduced weight for non-data-analysis strategies\n",
    "        'divergence': 0.10   # Reduced weight for non-data-analysis strategies\n",
    "    }\n",
    "    \n",
    "    # Dynamically adjust weights based on recent performance\n",
    "    if len(historical_df) >= 15:\n",
    "        # Track performance of each strategy\n",
    "        strategy_returns = {'datafc': 0, 'corr': 0, 'pair': 0, \n",
    "                            'hyper': 0, 'imbalance': 0, 'divergence': 0}\n",
    "        strategy_sharpes = {'datafc': 0, 'corr': 0, 'pair': 0, \n",
    "                           'hyper': 0, 'imbalance': 0, 'divergence': 0}\n",
    "        \n",
    "        # Backtest strategies over last 10 days\n",
    "        for i in range(1, min(10, len(historical_df))):\n",
    "            day_minus_1 = historical_df.iloc[-i-1]\n",
    "            day = historical_df.iloc[-i]\n",
    "            \n",
    "            # Price changes\n",
    "            r_tec = day['TEC'] / day_minus_1['TEC'] - 1\n",
    "            r_mkt = day['MKT'] / day_minus_1['MKT'] - 1\n",
    "            r_datafc = day['DATAFC'] / day_minus_1['DATAFC'] - 1\n",
    "            r_datafo = day['DATAFO'] / day_minus_1['DATAFO'] - 1\n",
    "            r_cash = 0.045 / 252  # Risk-free rate daily\n",
    "            \n",
    "            # Historical data slice\n",
    "            hist_df = historical_df.iloc[:-i]\n",
    "            if len(hist_df) > 1:\n",
    "                # Calculate weights for each strategy using historical data\n",
    "                try:\n",
    "                    # Data-driven strategies\n",
    "                    w_datafc = datafc_focused_strategy.__globals__['historical_df'] = hist_df\n",
    "                    w_datafc = datafc_focused_strategy()\n",
    "                    \n",
    "                    w_corr = correlation_weighted_strategy.__globals__['historical_df'] = hist_df\n",
    "                    w_corr = correlation_weighted_strategy()\n",
    "                    \n",
    "                    w_pair = enhanced_pair_trading_strategy.__globals__['historical_df'] = hist_df\n",
    "                    w_pair = enhanced_pair_trading_strategy()\n",
    "                    \n",
    "                    # Existing strategies\n",
    "                    w_hyper = hyper_optimized_asymmetric_strategy(hist_df)\n",
    "                    w_imbalance = optimized_imbalance_strategy(hist_df)\n",
    "                    w_divergence = optimized_imbalance_divergence_strategy(hist_df)\n",
    "                    \n",
    "                    # Calculate returns for all strategies\n",
    "                    for name, w in [\n",
    "                        ('datafc', w_datafc),\n",
    "                        ('corr', w_corr),\n",
    "                        ('pair', w_pair),\n",
    "                        ('hyper', w_hyper),\n",
    "                        ('imbalance', w_imbalance),\n",
    "                        ('divergence', w_divergence)\n",
    "                    ]:\n",
    "                        day_return = (\n",
    "                            w[0] * r_tec +\n",
    "                            w[1] * r_mkt +\n",
    "                            w[2] * r_datafc +\n",
    "                            w[3] * r_datafo +\n",
    "                            w[4] * r_cash\n",
    "                        )\n",
    "                        \n",
    "                        # Accumulate returns and squared returns for Sharpe calculation\n",
    "                        strategy_returns[name] += day_return\n",
    "                        strategy_sharpes[name] += day_return**2\n",
    "                except:\n",
    "                    # Fallback if any errors in historical calculation\n",
    "                    pass\n",
    "        \n",
    "        # Calculate Sharpe-like ratios\n",
    "        for name in strategy_sharpes:\n",
    "            if strategy_sharpes[name] > 0:\n",
    "                # Simple Sharpe approximation (return / volatility)\n",
    "                strategy_sharpes[name] = strategy_returns[name] / np.sqrt(strategy_sharpes[name])\n",
    "            else:\n",
    "                strategy_sharpes[name] = 0\n",
    "        \n",
    "        # Adjust weights based on performance, but maintain minimum for data-driven strategies\n",
    "        sharpe_sum = sum(max(0.001, s) for s in strategy_sharpes.values())\n",
    "        if sharpe_sum > 0:\n",
    "            # Temporary weights based on performance\n",
    "            temp_weights = {}\n",
    "            for name in strategy_weights:\n",
    "                # Calculate weight by performance\n",
    "                perf_weight = 0.05 + (0.7 * max(0, strategy_sharpes[name]) / sharpe_sum)\n",
    "                \n",
    "                # Apply floors for data-driven strategies\n",
    "                if name == 'datafc':\n",
    "                    temp_weights[name] = max(0.25, perf_weight)  # Minimum 25% for DATAFC strategy\n",
    "                elif name == 'corr':\n",
    "                    temp_weights[name] = max(0.20, perf_weight)  # Minimum 20% for correlation strategy\n",
    "                elif name == 'pair':\n",
    "                    temp_weights[name] = max(0.10, perf_weight)  # Minimum 10% for pair strategy\n",
    "                else:\n",
    "                    # Non-data-driven strategies, cap at 15%\n",
    "                    temp_weights[name] = min(0.15, perf_weight)\n",
    "            \n",
    "            # Normalize to ensure weights sum to 1.0\n",
    "            weight_sum = sum(temp_weights.values())\n",
    "            strategy_weights = {name: w/weight_sum for name, w in temp_weights.items()}\n",
    "    \n",
    "    # Combine strategies using optimized weights\n",
    "    ensemble_weights = (\n",
    "        strategy_weights['datafc'] * datafc_weights +\n",
    "        strategy_weights['corr'] * corr_weights +\n",
    "        strategy_weights['pair'] * pair_weights +\n",
    "        strategy_weights['hyper'] * hyper_weights +\n",
    "        strategy_weights['imbalance'] * imbalance_weights +\n",
    "        strategy_weights['divergence'] * divergence_weights\n",
    "    )\n",
    "    \n",
    "    # Final risk management\n",
    "    # 1. Ensure weights sum to 1.0\n",
    "    if not np.isclose(np.sum(ensemble_weights), 1.0):\n",
    "        ensemble_weights[4] = 1.0 - np.sum(ensemble_weights[:4])\n",
    "    \n",
    "    # 2. Check leverage constraint\n",
    "    leverage = np.sum(np.abs(ensemble_weights))\n",
    "    if leverage > 2.95:\n",
    "        # Scale non-cash components\n",
    "        scale_factor = 2.95 / leverage\n",
    "        for i in range(4):\n",
    "            ensemble_weights[i] *= scale_factor\n",
    "        \n",
    "        # Readjust cash\n",
    "        ensemble_weights[4] = 1.0 - np.sum(ensemble_weights[:4])\n",
    "    \n",
    "    return ensemble_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission & Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The criteria for grading the competition—based on CAGR and Sharpe ratio—naturally extend into two fundamental concepts in finance: portfolio optimization and optimal leverage. These concepts are deeply intertwined with the metrics used and encourage contestants to think about broader principles beyond simple return maximization (which is still important).\n",
    "\n",
    "**Portfolio Optimization (Markowitz Framework)**\n",
    "The use of the Sharpe ratio as a grading metric directly connects to the principles of portfolio optimization, as introduced by Harry Markowitz. In this framework, the goal is to construct a portfolio that maximizes return for a given level of risk or minimizes risk for a given level of return. Contestants are implicitly incentivized to consider not only the predictive accuracy of their strategies but also how their allocations manage risk across the portfolio. For instance, while a highly concentrated allocation might yield a high return, it could expose the portfolio to extreme volatility, reducing the Sharpe ratio. By balancing allocations between assets like SPY, QQQ, DATAFC, and DATAFO—and even the cash component—participants are effectively engaging in a simplified form of Markowitz optimization, seeking to allocate weights in a way that optimally balances risk and reward.\n",
    "\n",
    "**Optimal Leverage (Kelly Criterion)**\n",
    "The inclusion of CAGR as a grading criterion introduces the concept of optimal leverage, as informed by the Kelly Criterion. CAGR measures the long-term growth rate of the portfolio, reflecting the compounding effect of returns over time. To maximize CAGR, contestants must carefully consider the level of leverage in their strategies. While higher leverage can amplify returns, it also increases volatility and the risk of drawdowns, which can severely impact the compounding process. The Kelly Criterion provides a theoretical framework for determining the optimal leverage that maximizes long-term growth while avoiding overexposure to risk. Contestants who understand and apply this principle can find the balance between aggressive and conservative leverage, ensuring their strategies are sustainable over the long run.\n",
    "\n",
    "**Grading for the case** will be based on two key metrics: **compound annual growth rate (CAGR) and Sharpe ratio over one year of out-of-sample (OOS) data.** CAGR evaluates the absolute monetary performance of the strategy, reflecting its ability to maximize returns. The Sharpe ratio measures risk-adjusted returns, highlighting strategies that effectively balance profitability with volatility. While these metrics emphasize different aspects of performance, it is possible for a strategy to excel in all both by achieving high returns with consistent risk management and sustainable growth. Participants are encouraged to design robust strategies that optimize across these dimensions for a comprehensive evaluation.\n",
    "\n",
    "Your final score in the data science case will be determined by the sum of your placements between CAGR rankings and Sharpe ratio rankings. For example, if your team places 7th in CAGR and 2nd in Sharpe Ratio out of the 35 teams, your score will be 9. Lowest score wins.\n",
    "\n",
    "You will submit your trading strategy as a python file. Trying to gain an unfair advantage with outside data will result in immediate disqualification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements a grading function designed to evaluate trading algorithms based on historical data without allowing any \"look-ahead bias.\" Each day, the algorithm is provided with only the data available up to that point, ensuring fairness and adherence to real-world constraints. The function validates the contestant’s allocations against several rules: **allocations must sum to 1.0 and the sum of absolute values of positions cannot exceed 3 (to limit leverage).** It then calculates daily returns based on the provided weights and updates the portfolio's value. The key performance metrics final portfolio value and Sharpe ratio are computed to measure the effectiveness of the strategy. This same function will be applied to one year of out-of-sample (OOS) data to evaluate the robustness of the submitted algorithms under unseen market conditions. Please make sure that your submitted algorithm handles edge cases, such as the presence of only a single row at the beginning of the dataset. **Contestants are strongly encouraged to thoroughly test their code for bugs or violations of the constraints**, as submissions that fail to run correctly or violate the rules will be disqualified to ensure fairness for all participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_strategy(historical_df):\n",
    "    \"\"\"\n",
    "    Dummy strategy:\n",
    "    Returns equal weights (20% each) every day, including 'CASH'.\n",
    "    [w_TEC, w_MKT, w_DATAFC, w_DATAFO, w_CASH]\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    \n",
    "    return w\n",
    "\n",
    "def grade_submission(strategy_func, df, risk_free_rate=0.045):\n",
    "    \"\"\"\n",
    "    Evaluate a strategy by simulating over daily data (no look-ahead),\n",
    "    computing final portfolio value, CAGR, and Sharpe ratio.\n",
    "\n",
    "    Each day, we call `strategy_func(historical_df_up_to_today)`\n",
    "    so that the contestant can produce a separate allocation for that day.\n",
    "\n",
    "    Constraints:\n",
    "    1. Allocations must sum up to 1\n",
    "    2. The sum of absolute values of positions cannot exceed 3.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strategy_func : function\n",
    "        A function that takes a partial (historical) DataFrame:\n",
    "          df.iloc[:i+1], and returns an allocation array/list of 5 floats:\n",
    "          [w_TEC, w_MKT, w_DATAFC, w_DATAFO, w_CASH]\n",
    "\n",
    "    df : pd.DataFrame\n",
    "        Contains daily data sorted by 'Date' with columns:\n",
    "        [\n",
    "          'Date', 'TEC', 'MKT', 'DATAFC', 'DATAFO',\n",
    "          'TEC_VOLUME', 'MKT_VOLUME', 'DATAFC_VOLUME', 'DATAFO_VOLUME',\n",
    "          'MKT_IMBALANCE', 'TEC_IMBALANCE', 'DATAFO_IMBALANCE', 'DATAFC_IMBALANCE'\n",
    "        ]\n",
    "\n",
    "    risk_free_rate : float, optional\n",
    "        Risk-free rate per year (e.g. 0.02 for 2%). Defaults to 0.045 (4.5%).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    final_value : float\n",
    "        The final portfolio value after the last day.\n",
    "\n",
    "    cagr : float\n",
    "        Compound Annual Growth Rate:\n",
    "          final_value^(252 / number_of_trading_days) - 1\n",
    "\n",
    "    annual_sharpe : float\n",
    "        Annualized Sharpe Ratio:\n",
    "          (mean(daily_returns) - rf_daily) / std(daily_returns) * sqrt(252)\n",
    "\n",
    "    allocations_df : pd.DataFrame\n",
    "        A table showing each day's date and the weights used:\n",
    "        [\n",
    "          'Date',\n",
    "          'w_TEC', 'w_MKT', 'w_DATAFC', 'w_DATAFO', 'w_CASH'\n",
    "        ]\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    portfolio_value = 1.0\n",
    "    daily_returns = []\n",
    "    allocations_all_days = []\n",
    "\n",
    "    # Simulate from day 0 to day N-2 (because we look at day i -> i+1 prices)\n",
    "    for i in range(len(df) - 1):\n",
    "        # Provide historical data up to day i (inclusive): NO future data\n",
    "        historical_df = df.iloc[:i+1].copy()\n",
    "\n",
    "        # Contestant's strategy returns today's allocation\n",
    "        w = strategy_func(historical_df)\n",
    "\n",
    "        # Validate it sums to 1.0\n",
    "        if not np.isclose(np.sum(w), 1.0):\n",
    "            raise ValueError(f\"Allocations on day {i} do not sum to 1.0: {w}\")\n",
    "\n",
    "        # Validate leverage\n",
    "        total_leverage = np.sum(np.abs(w))\n",
    "        if total_leverage > 3:\n",
    "            raise ValueError(f\"Leverage exceeded 3 on day {i}: {total_leverage}\")\n",
    "\n",
    "        # Store the allocation along with today's date\n",
    "        allocations_all_days.append([\n",
    "            df.loc[i, 'Date'],\n",
    "            w[0],  # w_TEC\n",
    "            w[1],  # w_MKT\n",
    "            w[2],  # w_DATAFC\n",
    "            w[3],  # w_DATAFO\n",
    "            w[4],  # w_CASH\n",
    "        ])\n",
    "\n",
    "        # Calculate the daily return from day i to day i+1\n",
    "        r_tec    = df.loc[i+1, 'TEC']    / df.loc[i, 'TEC']    - 1\n",
    "        r_mkt    = df.loc[i+1, 'MKT']    / df.loc[i, 'MKT']    - 1\n",
    "        r_datafc = df.loc[i+1, 'DATAFC'] / df.loc[i, 'DATAFC'] - 1\n",
    "        r_datafo = df.loc[i+1, 'DATAFO'] / df.loc[i, 'DATAFO'] - 1\n",
    "\n",
    "        # Weights w = [w_TEC, w_MKT, w_DATAFC, w_DATAFO, w_CASH]\n",
    "        w_tec, w_mkt, w_datafc, w_datafo, w_cash = w\n",
    "        \n",
    "        # Sum up the returns for the day \n",
    "        day_return = (\n",
    "            w_tec    * r_tec +\n",
    "            w_mkt    * r_mkt +\n",
    "            w_datafc * r_datafc +\n",
    "            w_datafo * r_datafo + \n",
    "            w_cash * risk_free_rate / 252\n",
    "        )\n",
    "        \n",
    "        # Update the portfolio value\n",
    "        portfolio_value *= (1 + day_return)\n",
    "\n",
    "        # Check if portfolio value is negative\n",
    "        if portfolio_value < 0:\n",
    "            raise ValueError(f\"Portfolio value became negative on day {i}.\")\n",
    "        \n",
    "        daily_returns.append(day_return)\n",
    "\n",
    "    final_value = portfolio_value\n",
    "    n_days = len(daily_returns)\n",
    "    cagr = final_value**(252 / n_days) - 1\n",
    "\n",
    "    # Compute Sharpe Ratio\n",
    "    rf_daily = risk_free_rate / 252\n",
    "    excess_returns = [r - rf_daily for r in daily_returns]\n",
    "    avg_excess_return = np.mean(excess_returns)\n",
    "    std_excess_return = np.std(excess_returns, ddof=1)\n",
    "    if std_excess_return > 0:\n",
    "        annual_sharpe = (avg_excess_return / std_excess_return) * np.sqrt(252)\n",
    "    else:\n",
    "        annual_sharpe = 0.0\n",
    "\n",
    "    allocations_df = pd.DataFrame(\n",
    "        allocations_all_days,\n",
    "        columns=[\n",
    "            'Date',\n",
    "            'w_TEC',\n",
    "            'w_MKT',\n",
    "            'w_DATAFC',\n",
    "            'w_DATAFO',\n",
    "            'w_CASH'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return final_value, cagr, annual_sharpe, allocations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_data_driven_strategy(historical_df):\n",
    "    \"\"\"\n",
    "    Highly optimized data-driven strategy with precision position sizing,\n",
    "    adaptive risk allocation, and multiple safety mechanisms to ensure\n",
    "    leverage constraints are never violated.\n",
    "    \"\"\"\n",
    "    # Handle edge case of first day\n",
    "    if len(historical_df) < 2:\n",
    "        return np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    \n",
    "    # Get latest data\n",
    "    latest = historical_df.iloc[-1]\n",
    "    \n",
    "    # ----- ENHANCED FEATURE ENGINEERING -----\n",
    "    # 1. Create normalized imbalance features with cube root scaling to reduce outlier impact\n",
    "    norm_imb = {\n",
    "        'TEC': latest['TEC_IMBALANCE'] / (latest['TEC_VOLUME'] ** (1/3)),\n",
    "        'MKT': latest['MKT_IMBALANCE'] / (latest['MKT_VOLUME'] ** (1/3)),\n",
    "        'DATAFC': latest['DATAFC_IMBALANCE'] / (latest['DATAFC_VOLUME'] ** (1/3)),\n",
    "        'DATAFO': latest['DATAFO_IMBALANCE'] / (latest['DATAFO_VOLUME'] ** (1/3))\n",
    "    }\n",
    "    \n",
    "    # 2. Calculate z-scores with adaptive lookback periods based on asset volatility\n",
    "    z_scores = dict(norm_imb)\n",
    "    if len(historical_df) >= 10:\n",
    "        # Use longer lookback for less volatile assets\n",
    "        lookbacks = {\n",
    "            'TEC': min(25, len(historical_df)-1),\n",
    "            'MKT': min(30, len(historical_df)-1),  # MKT is typically less volatile\n",
    "            'DATAFC': min(20, len(historical_df)-1),\n",
    "            'DATAFO': min(20, len(historical_df)-1)\n",
    "        }\n",
    "        \n",
    "        for asset in norm_imb:\n",
    "            # Collect historical normalized imbalances with better exponential weighting\n",
    "            hist_values = []\n",
    "            weights = []\n",
    "            decay_factor = 0.92  # Slightly slower decay for more stability\n",
    "            \n",
    "            for i in range(1, lookbacks[asset]):\n",
    "                day = historical_df.iloc[-i]\n",
    "                hist_values.append(day[f'{asset}_IMBALANCE'] / (day[f'{asset}_VOLUME'] ** (1/3)))\n",
    "                weights.append(decay_factor**i)\n",
    "            \n",
    "            # Calculate weighted statistics with better numerical stability\n",
    "            if hist_values:\n",
    "                mean_val = np.average(hist_values, weights=weights)\n",
    "                weighted_var = max(0.001, np.average([(x - mean_val)**2 for x in hist_values], weights=weights))\n",
    "                std_val = np.sqrt(weighted_var)\n",
    "                # Convert to z-score with outlier capping\n",
    "                raw_zscore = (norm_imb[asset] - mean_val) / std_val\n",
    "                z_scores[asset] = np.clip(raw_zscore, -3.0, 3.0)  # Cap extreme values\n",
    "            else:\n",
    "                z_scores[asset] = np.clip(norm_imb[asset], -1.0, 1.0)\n",
    "    \n",
    "    # 3. Enhanced DATAFC/DATAFO ratio analysis with regime detection\n",
    "    fc_fo_ratio = latest['DATAFC'] / latest['DATAFO']\n",
    "    ratio_zscore = 0\n",
    "    ratio_regime = \"neutral\"  # Default regime\n",
    "    \n",
    "    if len(historical_df) >= 15:\n",
    "        # Calculate ratio z-score with decay-weighted statistics\n",
    "        ratio_series = historical_df['DATAFC'] / historical_df['DATAFO']\n",
    "        \n",
    "        # Exponential weighting with optimal decay\n",
    "        weights_exp = [0.94**i for i in range(min(25, len(ratio_series)-1))]\n",
    "        ratio_hist = ratio_series.iloc[-(len(weights_exp)+1):-1]\n",
    "        \n",
    "        if len(ratio_hist) > 0:\n",
    "            # Calculate weighted statistics\n",
    "            mean_ratio = np.average(ratio_hist, weights=weights_exp)\n",
    "            weighted_var = max(0.0001, np.average([(r - mean_ratio)**2 for r in ratio_hist], weights=weights_exp))\n",
    "            std_ratio = np.sqrt(weighted_var)\n",
    "            \n",
    "            # Z-score of current ratio\n",
    "            ratio_zscore = (fc_fo_ratio - mean_ratio) / std_ratio\n",
    "            \n",
    "            # Detect regime for adaptive position sizing\n",
    "            if len(ratio_hist) >= 10:\n",
    "                # Calculate autocorrelation to detect mean-reversion vs trend\n",
    "                ratio_changes = ratio_hist.pct_change().dropna()\n",
    "                if len(ratio_changes) >= 3:\n",
    "                    autocorr = ratio_changes.autocorr() if len(ratio_changes) > 1 else 0\n",
    "                    \n",
    "                    if autocorr < -0.2:\n",
    "                        ratio_regime = \"mean_reverting\"  # Stronger mean-reversion\n",
    "                    elif autocorr > 0.2:\n",
    "                        ratio_regime = \"trending\"        # Weaker mean-reversion\n",
    "    \n",
    "    # 4. Market environment detection\n",
    "    market_regime = \"neutral\"\n",
    "    if len(historical_df) >= 20:\n",
    "        # Calculate recent index volatility as market regime indicator\n",
    "        idx_returns = (historical_df['TEC'].pct_change() + historical_df['MKT'].pct_change()) / 2\n",
    "        recent_vol = idx_returns.iloc[-20:].std() * np.sqrt(252)\n",
    "        \n",
    "        # Compare to longer-term average\n",
    "        if len(historical_df) >= 40:\n",
    "            longer_vol = idx_returns.iloc[-40:-20].std() * np.sqrt(252)\n",
    "            vol_ratio = recent_vol / longer_vol if longer_vol > 0 else 1.0\n",
    "            \n",
    "            if vol_ratio > 1.3:\n",
    "                market_regime = \"high_volatility\"\n",
    "            elif vol_ratio < 0.7:\n",
    "                market_regime = \"low_volatility\"\n",
    "    \n",
    "    # ----- OPTIMIZED PORTFOLIO CONSTRUCTION -----\n",
    "    # Initialize weights with dynamic cash buffer based on market regime\n",
    "    base_cash = 0.15\n",
    "    if market_regime == \"high_volatility\":\n",
    "        base_cash = 0.25  # More defensive in high volatility\n",
    "    \n",
    "    weights = {'TEC': 0, 'MKT': 0, 'DATAFC': 0, 'DATAFO': 0, 'CASH': base_cash}\n",
    "    \n",
    "    # Define correlation coefficients from data analysis\n",
    "    correlations = {\n",
    "        'DATAFC': 0.4673,  # Strongest predictive power\n",
    "        'TEC': 0.1623,\n",
    "        'MKT': 0.1566,\n",
    "        'DATAFO': 0.1279   # Weakest predictive power\n",
    "    }\n",
    "    \n",
    "    # 1. OPTIMAL RISK BUDGET ALLOCATION\n",
    "    # Calculate total signal quality score (correlation * signal strength)\n",
    "    total_signal = 0\n",
    "    signal_quality = {}\n",
    "    \n",
    "    for asset, corr in correlations.items():\n",
    "        # Scale by both correlation and z-score magnitude\n",
    "        strength = abs(z_scores[asset])\n",
    "        quality = corr * min(1.5, strength)\n",
    "        signal_quality[asset] = quality\n",
    "        total_signal += quality\n",
    "    \n",
    "    # Available risk budget (leverage) after cash allocation\n",
    "    available_leverage = 2.7  # Conservative target maximum leverage\n",
    "    \n",
    "    # 2. CORE POSITION SIZING WITH PROPORTIONAL ALLOCATION\n",
    "    # Track allocated leverage\n",
    "    allocated_leverage = 0\n",
    "    \n",
    "    # First allocation pass - proportional to signal quality\n",
    "    if total_signal > 0:\n",
    "        for asset, quality in signal_quality.items():\n",
    "            # Skip assets with weak signals\n",
    "            if abs(z_scores[asset]) < 0.8:\n",
    "                continue\n",
    "                \n",
    "            # Calculate ideal position size proportional to signal quality\n",
    "            ideal_size = (available_leverage * 0.7) * (quality / total_signal)\n",
    "            # Scale down for weaker signals\n",
    "            actual_size = ideal_size * min(1.0, abs(z_scores[asset]) / 1.5)\n",
    "            # Cap individual position size\n",
    "            actual_size = min(actual_size, 0.7)  # Maximum 70% in any asset\n",
    "            \n",
    "            # Only take position if meaningful\n",
    "            if actual_size > 0.05:\n",
    "                weights[asset] = actual_size * np.sign(z_scores[asset])\n",
    "                allocated_leverage += actual_size\n",
    "    \n",
    "    # LEVERAGE CHECK #1: After initial allocation\n",
    "    current_leverage = sum(abs(w) for w in weights.values())\n",
    "    if current_leverage > 2.4:\n",
    "        scale_factor = 2.4 / current_leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= scale_factor\n",
    "        allocated_leverage *= scale_factor\n",
    "    \n",
    "    # 3. ENHANCED PAIR TRADING ALLOCATION\n",
    "    # Calculate remaining leverage budget\n",
    "    remaining_leverage = 2.6 - sum(abs(w) for w in weights.values())\n",
    "    \n",
    "    # Only apply pair trade if ratio signal is meaningful and we have leverage budget\n",
    "    if abs(ratio_zscore) > 1.2 and remaining_leverage > 0.3:\n",
    "        # Adaptive pair size based on regime\n",
    "        if ratio_regime == \"mean_reverting\":\n",
    "            # Stronger size in mean-reverting regimes\n",
    "            base_pair_size = 0.15\n",
    "        elif ratio_regime == \"trending\":\n",
    "            # Smaller size in trending regimes\n",
    "            base_pair_size = 0.08\n",
    "        else:\n",
    "            # Default size in neutral regimes\n",
    "            base_pair_size = 0.12\n",
    "        \n",
    "        # Scale by zscore strength but maintain reasonable size\n",
    "        pair_size = base_pair_size * min(1.0, abs(ratio_zscore) / 2.0)\n",
    "        # Ensure we don't exceed remaining leverage\n",
    "        pair_size = min(pair_size, remaining_leverage / 2.0)\n",
    "        \n",
    "        # Apply pair trade\n",
    "        if ratio_zscore > 0:  # DATAFC expensive relative to DATAFO\n",
    "            weights['DATAFC'] -= pair_size\n",
    "            weights['DATAFO'] += pair_size\n",
    "        else:  # DATAFO expensive relative to DATAFC\n",
    "            weights['DATAFC'] += pair_size\n",
    "            weights['DATAFO'] -= pair_size\n",
    "            \n",
    "        # Update allocated leverage\n",
    "        allocated_leverage += (2 * pair_size)\n",
    "    \n",
    "    # LEVERAGE CHECK #2: After pair trading\n",
    "    current_leverage = sum(abs(w) for w in weights.values())\n",
    "    if current_leverage > 2.6:\n",
    "        scale_factor = 2.6 / current_leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= scale_factor\n",
    "    \n",
    "    # ----- PRECISION RISK MANAGEMENT -----\n",
    "    # 1. Finalize cash allocation based on current leverage\n",
    "    non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "    weights['CASH'] = 1.0 - non_cash_sum\n",
    "    \n",
    "    # 2. Final leverage check with numerical precision\n",
    "    final_leverage = sum(abs(w) for w in weights.values())\n",
    "    \n",
    "    # Apply scaling with buffer if too close to limit\n",
    "    if final_leverage > 2.85:\n",
    "        # Use a more conservative scaling to account for numerical precision\n",
    "        scale_factor = 2.85 / final_leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= scale_factor\n",
    "        \n",
    "        # Readjust cash\n",
    "        non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "        weights['CASH'] = 1.0 - non_cash_sum\n",
    "    \n",
    "    # 3. Final numerical integrity check with epsilon buffer\n",
    "    epsilon = 1e-10\n",
    "    if not np.isclose(sum(weights.values()), 1.0, rtol=epsilon, atol=epsilon):\n",
    "        # Force cash to make sum exactly 1.0\n",
    "        weights['CASH'] = 1.0 - sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "    \n",
    "    # 4. Final leverage safety check with strict enforcement\n",
    "    final_final_leverage = sum(abs(w) for w in weights.values())\n",
    "    if final_final_leverage >= 2.99:\n",
    "        # Emergency fallback allocation\n",
    "        return np.array([0.225, 0.225, 0.225, 0.225, 0.1])\n",
    "    \n",
    "    return np.array([\n",
    "        weights['TEC'],\n",
    "        weights['MKT'],\n",
    "        weights['DATAFC'],\n",
    "        weights['DATAFO'],\n",
    "        weights['CASH']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final portfolio value: 31.518873428443467\n",
      "CAGR:                 465.29%\n",
      "Annual Sharpe ratio:  6.87\n",
      "\n",
      "Daily Allocations:\n",
      "         Date     w_TEC     w_MKT  w_DATAFC  w_DATAFO    w_CASH\n",
      "0  2024-01-03  0.200000  0.200000  0.200000  0.200000  0.200000\n",
      "1  2024-01-04  0.335573  0.323787 -0.700000  0.264447  0.776193\n",
      "2  2024-01-05 -0.335573 -0.323787  0.700000  0.264447  0.694913\n",
      "3  2024-01-08  0.313817 -0.302796 -0.654619  0.247303  1.396295\n",
      "4  2024-01-09 -0.335573  0.323787  0.700000  0.264447  0.047338\n",
      "5  2024-01-10  0.335573  0.323787 -0.700000  0.264447  0.776193\n",
      "6  2024-01-11 -0.335573  0.323787  0.700000 -0.264447  0.576232\n",
      "7  2024-01-12  0.225000  0.225000  0.225000  0.225000  0.100000\n",
      "8  2024-01-16  0.335573  0.323787  0.700000  0.264447 -0.623807\n",
      "9  2024-01-17  0.000000  0.000000  0.000000  0.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Run grading\n",
    "final_val, cagr_val, sharpe_val, allocations_df = grade_submission(optimized_data_driven_strategy, auction_prices)\n",
    "\n",
    "print(\"Final portfolio value:\", final_val)\n",
    "print(\"CAGR:                \", f\"{cagr_val:.2%}\")\n",
    "print(\"Annual Sharpe ratio: \", f\"{sharpe_val:.2f}\")\n",
    "print(\"\\nDaily Allocations:\")\n",
    "print(allocations_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
