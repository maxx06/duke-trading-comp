{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 160\n",
      "-rw-r--r--@ 1 max  staff  23050 Mar 30 01:09 DTC_case_study (1).ipynb\n",
      "-rw-r--r--@ 1 max  staff  56974 Mar 30 09:36 auction_prices.csv\n"
     ]
    }
   ],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date     TEC     MKT  DATAFC  DATAFO  TEC_VOLUME  MKT_VOLUME  \\\n",
      "0  2024-01-03  548.29  633.26   28.61   28.51    27197400    96582300   \n",
      "1  2024-01-04  546.47  633.68   28.75   28.33    23674700    90683400   \n",
      "2  2024-01-05  548.23  635.40   28.21   28.03    38008900   102026400   \n",
      "3  2024-01-08  540.13  626.95   28.24   28.21    48842300   149892000   \n",
      "4  2024-01-09  550.43  633.78   28.41   28.08    37178900   105016100   \n",
      "\n",
      "   DATAFC_VOLUME  DATAFO_VOLUME  MKT_IMBALANCE  TEC_IMBALANCE  \\\n",
      "0       89610300       90036218       -32638.0     -1653817.0   \n",
      "1       83296620       83692529      8961901.0       832427.0   \n",
      "2       85654260       86061375     -5676197.0     -1017302.0   \n",
      "3       96659244       97118665     -2958468.0      2150464.0   \n",
      "4       99204696       99676216      1565000.0      -708564.0   \n",
      "\n",
      "   DATAFO_IMBALANCE  DATAFC_IMBALANCE  \n",
      "0         -479551.0         -482876.0  \n",
      "1           72074.0        -1425029.0  \n",
      "2          291916.0          629758.0  \n",
      "3          524343.0        -1174935.0  \n",
      "4          620513.0         1852407.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "auction_prices = pd.read_csv('auction_prices.csv')\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "print(auction_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'TEC', 'MKT', 'DATAFC', 'DATAFO', 'TEC_VOLUME', 'MKT_VOLUME',\n",
      "       'DATAFC_VOLUME', 'DATAFO_VOLUME', 'MKT_IMBALANCE', 'TEC_IMBALANCE',\n",
      "       'DATAFO_IMBALANCE', 'DATAFC_IMBALANCE'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 503 entries, 0 to 502\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Date              503 non-null    object \n",
      " 1   TEC               503 non-null    float64\n",
      " 2   MKT               503 non-null    float64\n",
      " 3   DATAFC            503 non-null    float64\n",
      " 4   DATAFO            503 non-null    float64\n",
      " 5   TEC_VOLUME        503 non-null    int64  \n",
      " 6   MKT_VOLUME        503 non-null    int64  \n",
      " 7   DATAFC_VOLUME     503 non-null    int64  \n",
      " 8   DATAFO_VOLUME     503 non-null    int64  \n",
      " 9   MKT_IMBALANCE     503 non-null    float64\n",
      " 10  TEC_IMBALANCE     503 non-null    float64\n",
      " 11  DATAFO_IMBALANCE  503 non-null    float64\n",
      " 12  DATAFC_IMBALANCE  503 non-null    float64\n",
      "dtypes: float64(8), int64(4), object(1)\n",
      "memory usage: 51.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display column names\n",
    "print(auction_prices.columns)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(auction_prices.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.4)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/max/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/max/Library/Python/3.11/lib/python/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/max/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-macosx_11_0_arm64.whl (254 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pyparsing, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pandas-2.2.3 pyparsing-3.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an alternative reality, the era of continuous trading came to an abrupt end in the early 2010s in the aftermath of the Great Recession, replaced by a single daily auction that redefined global financial markets. The shift was not the product of a single event but rather the culmination of decades of discontent with the trajectory of modern finance. Public sentiment had turned against the financial industry, increasingly viewed as a system that prioritized speed and speculation over substance and societal contribution. Stories of brilliant minds dedicating their careers to shaving microseconds off trade execution times—rather than solving pressing global challenges—fueled a growing belief that finance had become a profound waste of human potential. This narrative gained traction in the public consciousness and became a rallying cry for reform.\n",
    "\n",
    "Behind the scenes, a quieter but equally consequential shift was taking place. Traditional stock exchanges, long the gatekeepers of market activity, found themselves losing ground to alternative trading venues like dark pools. Nearly half of all trading volume occurred in these opaque, off-exchange platforms, where institutional investors could transact large blocks of shares without moving the market. Frustrated by this erosion of their influence and revenue, exchanges began lobbying regulators for a radical restructuring of the market. Their argument was ostensibly about fairness and transparency, but critics noted that the proposed system of a single daily auction would concentrate trading volume—and fees—back into the exchanges’ hands. This alignment of public frustration and private lobbying created the conditions for sweeping change.\n",
    "\n",
    "The transition to the single auction system was implemented over a period of five years, during which market participants were forced to adapt to the new paradigm. Each day, orders could be submitted until a predetermined cutoff time, after which prices were set in a centralized auction. Exchanges marketed the system as a triumph of simplicity and fairness, claiming it would eliminate the chaos of continious trading and fragmentation. For the public, the promise of a less frenetic market resonated with a desire to see finance serve the broader economy rather than itself. However, many skeptics saw the reform as a power grab by exchanges, who now controlled the critical moment of price discovery.\n",
    "\n",
    "As the system took hold, its impact was profound. Liquidity during the rest of the day dried up; the absence of continuous trading reshaped the role of finance professionals, shifting emphasis from speed and execution to analysis and forecasting. While some hailed the changes as a return to fundamentals, others lamented the loss of opportunities for real-time price adjustments. Outside the financial sector, opinions were equally divided. Advocates pointed to the reduction in volatility and the restoration of trust in the markets, arguing that the auction system had lowered the cost of trading since there was no longer a bid-ask spread. Detractors, however, noted that the concentration of trading into a single moment increased systemic risks and sometimes resulted in large price swings. In hindsight, the move to single daily auctions was both a product of its time and a reflection of deeper societal shifts. It addressed many of the immediate concerns around market stability and transparency, yet it left lingering questions about the role of finance in the broader human endeavor. Whether it represented genuine progress or merely a consolidation of power remains a topic of debate among historians and economists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auction Reference Price** \n",
    "\n",
    "The auction reference price is the price at which the highest number of buy and sell orders can be matched during an auction, ensuring maximum trade execution. It serves as the clearing price where supply and demand are balanced and is also the price at which market participants can both buy and sell without incurring additional costs. This price is provided to you under the ticker in the auction dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auction Volume** \n",
    "\n",
    "Auction volume measures the total quantity of an asset traded during a specific auction. This data is reported to participants before the auction close, so **you see volume info for the upcoming auction before it finalizes**. This reflects the aggregate demand and supply participating in the auction at a given time. High auction volume typically indicates significant market interest and liquidity, as institutional and retail traders align their trades to the auction, aiming to achieve execution at a fair and representative price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auction Imbalance**\n",
    "\n",
    "Auction imbalance refers to the difference between the total buy and sell orders submitted for an asset in a scheduled auction, highlighting whether demand or supply dominates. The imbalance data is reported to participants before the auction close, so **you see imbalance info for the upcoming auction before it finalizes**. It serves as an indicator of market sentiment and helps market participants anticipate price movements. A positive imbalance suggests higher buying interest, potentially driving the auction price upward, while a negative imbalance indicates more selling pressure, likely pushing prices lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Market Composite Index (MKT)**\n",
    "\n",
    "The Market Composite Index (MKT) is a benchmark tracking the performance of a diversified portfolio of stocks across various sectors and market capitalizations. Representing a broad cross-section of the economy, this index includes companies from established industries such as healthcare and finance, as well as dynamic sectors like technology and energy. Designed to reflect the overall health of the equity markets, the MKT Index serves as a critical reference point for both individual investors and institutional funds.\n",
    "\n",
    "With moderate volatility, the MKT Index provides a balanced view of market trends, blending the stability of blue-chip stocks with the growth potential of emerging companies. Its performance is closely tied to macroeconomic indicators such as GDP, inflation, and interest rates, offering a snapshot of broader market sentiment. Ideal for long-term investors seeking a diversified market benchmark, the MKT Index remains a cornerstone of portfolio construction and performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tech Evolution Index (TEC)**\n",
    "\n",
    "The Tech Evolution Index (TEC) offers concentrated exposure to the high-growth technology sector, encompassing companies leading innovation in software, hardware, semiconductors, and digital services. This index is a barometer for the performance of technology-driven industries, tracking trends in artificial intelligence, cloud computing, and cybersecurity. TEC is widely regarded as an indicator of growth within the global economy's most dynamic sector.\n",
    "\n",
    "Known for its higher volatility compared to broader indices, the Tech Evolution Index attracts investors seeking to capitalize on technology’s rapid evolution and outsized economic contributions. The index is influenced by tech earnings reports, innovation cycles, and sector-specific developments like regulatory changes or global supply chain dynamics. For those looking to align their portfolios with cutting-edge innovation, TEC provides a focused lens on technology's market potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataForge Analytics Class A (DATAFO)**\n",
    "\n",
    "DataForge Analytics Class A (DATAFO) represents the voting shares of DataForge Analytics, a leader in enterprise data solutions, artificial intelligence, and cloud-based services. Designed for investors who value governance participation, DATAFO offers voting rights, allowing shareholders to influence corporate strategies and key decisions. This makes it an attractive option for institutional investors focused on long-term engagement.\n",
    "\n",
    "DATAFO reflects the company’s growth trajectory as it expands its footprint in enterprise analytics and digital transformation. Performance drivers include earnings results, customer acquisition, and industry partnerships, with external factors like advancements in AI and data privacy regulations adding complexity. With its strong market position and innovative product pipeline, DATAFO is ideal for investors seeking exposure to the data economy’s growing relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataForge Analytics Class C (DATAFC)**\n",
    "\n",
    "DataForge Analytics Class C (DATAFC) provides a non-voting alternative to DataForge's Class A shares, offering investors a cost-effective way to participate in the company’s financial success. While DATAFC lacks governance rights, it is designed for those prioritizing capital appreciation over corporate influence. As such, it sometimes trades at a slight discount to DATAFO but shares the same exposure to the company’s financial performance.\n",
    "\n",
    "Popular among active traders and passive investors alike, DATAFC combines liquidity with the opportunity to benefit from DataForge’s leadership in AI and data-driven solutions. Its performance mirrors that of DATAFO, driven by earnings growth, product innovation, and market trends in data analytics. For investors seeking affordable access to a tech-forward company poised for long-term growth, DATAFC is an excellent choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis and algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_imbalance_strategy(historical_df):\n",
    "    \"\"\"\n",
    "    Optimized imbalance-based momentum strategy that:\n",
    "    1. Calculates normalized imbalance (imbalance/volume) for each asset\n",
    "    2. Ranks assets by imbalance strength\n",
    "    3. Allocates weights based on ranking with dynamic adjustments\n",
    "    4. Incorporates volume changes and volatility-based position sizing\n",
    "    5. Maximizes leverage utilization for strong signals\n",
    "    \"\"\"\n",
    "    # Get the latest day's data\n",
    "    latest = historical_df.iloc[-1]\n",
    "    \n",
    "    # Calculate normalized imbalances\n",
    "    tec_norm_imb = latest['TEC_IMBALANCE'] / latest['TEC_VOLUME']\n",
    "    mkt_norm_imb = latest['MKT_IMBALANCE'] / latest['MKT_VOLUME']\n",
    "    datafc_norm_imb = latest['DATAFC_IMBALANCE'] / latest['DATAFC_VOLUME']\n",
    "    datafo_norm_imb = latest['DATAFO_IMBALANCE'] / latest['DATAFO_VOLUME']\n",
    "    \n",
    "    # Create list of tuples (asset, normalized imbalance)\n",
    "    assets = [\n",
    "        ('TEC', tec_norm_imb),\n",
    "        ('MKT', mkt_norm_imb),\n",
    "        ('DATAFC', datafc_norm_imb),\n",
    "        ('DATAFO', datafo_norm_imb)\n",
    "    ]\n",
    "    \n",
    "    # Sort by normalized imbalance (highest to lowest)\n",
    "    assets.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Initialize weights dictionary with default values\n",
    "    weights = {\n",
    "        'TEC': 0,\n",
    "        'MKT': 0,\n",
    "        'DATAFC': 0,\n",
    "        'DATAFO': 0,\n",
    "        'CASH': 0.15  # Default cash buffer\n",
    "    }\n",
    "    \n",
    "    # Dynamic cash adjustment based on volatility\n",
    "    if len(historical_df) >= 10:\n",
    "        # Calculate recent volatility\n",
    "        recent_returns = historical_df[['TEC', 'MKT', 'DATAFC', 'DATAFO']].pct_change().iloc[-10:].std()\n",
    "        avg_vol = recent_returns.mean()\n",
    "        \n",
    "        # Adjust cash based on volatility (between 10-30%)\n",
    "        weights['CASH'] = min(0.3, max(0.1, avg_vol * 10))\n",
    "    \n",
    "    # Assign weights based on ranking and signal strength\n",
    "    if len(assets) >= 1:\n",
    "        # Highest ranked asset\n",
    "        if abs(assets[0][1]) > 0.01:  # Very strong signal\n",
    "            weights[assets[0][0]] = 1.0\n",
    "            weights['CASH'] = 0.1  # Reduce cash to minimum\n",
    "        elif abs(assets[0][1]) > 0.005:  # Strong signal\n",
    "            weights[assets[0][0]] = 0.6\n",
    "        else:  # Moderate signal\n",
    "            weights[assets[0][0]] = 0.5\n",
    "    \n",
    "    if len(assets) >= 2:\n",
    "        # Second ranked asset\n",
    "        if abs(assets[1][1]) > 0.005:  # Strong signal\n",
    "            weights[assets[1][0]] = 0.3\n",
    "        else:  # Moderate signal\n",
    "            weights[assets[1][0]] = 0.25\n",
    "    \n",
    "    if len(assets) >= 3:\n",
    "        # Third ranked asset\n",
    "        weights[assets[2][0]] = 0.1\n",
    "    \n",
    "    if len(assets) >= 4:\n",
    "        # Lowest ranked asset (short position)\n",
    "        if assets[3][1] < -0.005:  # Strong negative signal\n",
    "            weights[assets[3][0]] = -0.2\n",
    "        else:  # Moderate negative signal\n",
    "            weights[assets[3][0]] = -0.1\n",
    "    \n",
    "    # Volume-based signal reinforcement\n",
    "    if len(historical_df) >= 2:\n",
    "        for asset_name, _ in assets:\n",
    "            volume_col = f\"{asset_name}_VOLUME\"\n",
    "            if volume_col in historical_df.columns:\n",
    "                volume_ratio = latest[volume_col] / historical_df[volume_col].iloc[-2]\n",
    "                \n",
    "                # Strengthen signals with volume confirmation\n",
    "                if volume_ratio > 1.2 and weights[asset_name] > 0:  # Volume spike, long position\n",
    "                    weights[asset_name] *= 1.2\n",
    "                elif volume_ratio > 1.2 and weights[asset_name] < 0:  # Volume spike, short position\n",
    "                    weights[asset_name] *= 1.3  # Strengthen short on volume confirmation\n",
    "    \n",
    "    # Ensure sum of weights equals 1.0\n",
    "    weight_sum = sum(weights.values())\n",
    "    if not np.isclose(weight_sum, 1.0):\n",
    "        # Adjust cash to make weights sum to 1.0\n",
    "        weights['CASH'] += (1.0 - weight_sum)\n",
    "    \n",
    "    # Validate leverage constraint (sum of abs values <= 3)\n",
    "    total_leverage = sum(abs(w) for w in weights.values())\n",
    "    if total_leverage > 3:\n",
    "        # Scale down proportionally\n",
    "        scale_factor = 3 / total_leverage\n",
    "        for asset in weights:\n",
    "            weights[asset] *= scale_factor\n",
    "        \n",
    "        # Ensure sum is still 1.0 after scaling\n",
    "        weights['CASH'] += (1.0 - sum(weights.values()))\n",
    "    \n",
    "    # Return weights in the required order\n",
    "    return np.array([\n",
    "        weights['TEC'],\n",
    "        weights['MKT'],\n",
    "        weights['DATAFC'],\n",
    "        weights['DATAFO'],\n",
    "        weights['CASH']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_asymmetric_liquidity_strategy(historical_df):\n",
    "    \"\"\"\n",
    "    Optimized asymmetric liquidity strategy with enhanced signal processing,\n",
    "    adaptive position sizing, and strict leverage controls.\n",
    "    \"\"\"\n",
    "    # Handle edge case of first day\n",
    "    if len(historical_df) < 2:\n",
    "        return np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    \n",
    "    # Get the latest day's data\n",
    "    latest = historical_df.iloc[-1]\n",
    "    \n",
    "    # ----- ENHANCED SIGNAL PROCESSING -----\n",
    "    # 1. Calculate normalized imbalance ratios with volume scaling\n",
    "    # Use square root of volume to reduce impact of extreme volume days\n",
    "    imb_ratios = {\n",
    "        'TEC': latest['TEC_IMBALANCE'] / (latest['TEC_VOLUME'] ** 0.5),\n",
    "        'MKT': latest['MKT_IMBALANCE'] / (latest['MKT_VOLUME'] ** 0.5),\n",
    "        'DATAFC': latest['DATAFC_IMBALANCE'] / (latest['DATAFC_VOLUME'] ** 0.5),\n",
    "        'DATAFO': latest['DATAFO_IMBALANCE'] / (latest['DATAFO_VOLUME'] ** 0.5)\n",
    "    }\n",
    "    \n",
    "    # 2. Calculate z-scores if we have enough history (improves signal quality)\n",
    "    if len(historical_df) >= 10:\n",
    "        for asset in ['TEC', 'MKT', 'DATAFC', 'DATAFO']:\n",
    "            # Get historical normalized imbalances\n",
    "            hist_imb = []\n",
    "            for i in range(min(10, len(historical_df))):\n",
    "                day = historical_df.iloc[-(i+1)]\n",
    "                hist_imb.append(day[f'{asset}_IMBALANCE'] / (day[f'{asset}_VOLUME'] ** 0.5))\n",
    "            \n",
    "            # Calculate mean and std of historical values\n",
    "            mean_imb = np.mean(hist_imb)\n",
    "            std_imb = np.std(hist_imb) if np.std(hist_imb) > 0 else 1.0\n",
    "            \n",
    "            # Convert current ratio to z-score\n",
    "            imb_ratios[asset] = (imb_ratios[asset] - mean_imb) / std_imb\n",
    "    \n",
    "    # 3. Calculate relative cross-asset signal strength\n",
    "    # This helps identify assets with strongest relative imbalances\n",
    "    avg_imb_ratio = sum(imb_ratios.values()) / len(imb_ratios)\n",
    "    relative_imb = {asset: ratio - avg_imb_ratio for asset, ratio in imb_ratios.items()}\n",
    "    \n",
    "    # 4. Compute signal reliability score based on consistency\n",
    "    signal_reliability = {asset: 1.0 for asset in imb_ratios}  # Default value\n",
    "    if len(historical_df) >= 5:\n",
    "        for asset in imb_ratios:\n",
    "            # Check imbalance consistency (same direction) over past 3 days\n",
    "            consistent_count = 1  # Start with 1 for current day\n",
    "            current_sign = np.sign(latest[f'{asset}_IMBALANCE'])\n",
    "            for i in range(1, min(3, len(historical_df))):\n",
    "                prev_sign = np.sign(historical_df.iloc[-(i+1)][f'{asset}_IMBALANCE'])\n",
    "                if current_sign == prev_sign:\n",
    "                    consistent_count += 1\n",
    "            \n",
    "            # Higher reliability for consistent signals\n",
    "            signal_reliability[asset] = 0.8 + (0.2 * consistent_count / 3)\n",
    "    \n",
    "    # 5. Compute final combined signal\n",
    "    combined_signals = {}\n",
    "    for asset in imb_ratios:\n",
    "        # Weight absolute and relative signals, adjusted by reliability\n",
    "        combined_signals[asset] = ((0.6 * imb_ratios[asset]) + \n",
    "                                  (0.4 * relative_imb[asset])) * signal_reliability[asset]\n",
    "    \n",
    "    # Sort assets by combined signal (descending)\n",
    "    sorted_assets = sorted(combined_signals.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # ----- OPTIMIZED POSITION SIZING -----\n",
    "    # Initialize weights\n",
    "    weights = {'TEC': 0, 'MKT': 0, 'DATAFC': 0, 'DATAFO': 0, 'CASH': 0.15}\n",
    "    \n",
    "    # Calculate signal strength scores (0-1 range) for position sizing\n",
    "    max_signal = max(abs(s) for _, s in sorted_assets)\n",
    "    if max_signal > 0:\n",
    "        signal_strength = {asset: min(1.0, abs(signal)/max_signal) for asset, signal in sorted_assets}\n",
    "    else:\n",
    "        signal_strength = {asset: 0.5 for asset, _ in sorted_assets}  # Default if no signals\n",
    "    \n",
    "    # Position sizing based on rank and signal strength\n",
    "    # First asset - strongest signal\n",
    "    top_asset = sorted_assets[0][0]\n",
    "    top_signal = sorted_assets[0][1]\n",
    "    top_strength = signal_strength[top_asset]\n",
    "    \n",
    "    # Dynamic weight based on signal direction and strength\n",
    "    if top_signal > 0:\n",
    "        weights[top_asset] = 0.5 + (0.1 * top_strength)  # 0.5-0.6 range for longs\n",
    "    else:\n",
    "        weights[top_asset] = -0.3 * top_strength  # 0 to -0.3 range for shorts\n",
    "    \n",
    "    # Second asset\n",
    "    second_asset = sorted_assets[1][0]\n",
    "    second_signal = sorted_assets[1][1]\n",
    "    second_strength = signal_strength[second_asset]\n",
    "    \n",
    "    if second_signal > 0:\n",
    "        weights[second_asset] = 0.25 + (0.05 * second_strength)  # 0.25-0.3 range\n",
    "    else:\n",
    "        weights[second_asset] = -0.15 * second_strength  # 0 to -0.15 range\n",
    "    \n",
    "    # Third asset - only take meaningful position if signal is strong\n",
    "    third_asset = sorted_assets[2][0]\n",
    "    third_signal = sorted_assets[2][1]\n",
    "    third_strength = signal_strength[third_asset]\n",
    "    \n",
    "    if abs(third_signal) > 0.5:  # Only take position if signal is meaningful\n",
    "        if third_signal > 0:\n",
    "            weights[third_asset] = 0.1 + (0.05 * third_strength)  # 0.1-0.15 range\n",
    "        else:\n",
    "            weights[third_asset] = -0.05 * third_strength  # 0 to -0.05 range\n",
    "    \n",
    "    # Fourth asset - only short if very negative\n",
    "    fourth_asset = sorted_assets[3][0]\n",
    "    fourth_signal = sorted_assets[3][1]\n",
    "    if fourth_signal < -1.0:  # Only short if signal is strongly negative\n",
    "        weights[fourth_asset] = -0.05  # Small short position\n",
    "    \n",
    "    # Pair relationship between DATAFC and DATAFO (same company different share classes)\n",
    "    # Exploit relative value opportunities between the two share classes\n",
    "    if len(historical_df) >= 10:\n",
    "        # Calculate typical price ratio\n",
    "        ratio_series = historical_df['DATAFC'] / historical_df['DATAFO']\n",
    "        mean_ratio = ratio_series.iloc[-10:].mean()\n",
    "        current_ratio = latest['DATAFC'] / latest['DATAFO']\n",
    "        \n",
    "        # If DATAFC is cheap relative to DATAFO, shift some weight to DATAFC\n",
    "        if current_ratio < 0.98 * mean_ratio and weights['DATAFC'] >= 0:\n",
    "            pair_adjustment = min(0.05, weights['DATAFO'] * 0.2)\n",
    "            weights['DATAFC'] += pair_adjustment\n",
    "            weights['DATAFO'] -= pair_adjustment\n",
    "        \n",
    "        # If DATAFO is cheap relative to DATAFC, shift some weight to DATAFO\n",
    "        elif current_ratio > 1.02 * mean_ratio and weights['DATAFO'] >= 0:\n",
    "            pair_adjustment = min(0.05, weights['DATAFC'] * 0.2)\n",
    "            weights['DATAFC'] -= pair_adjustment\n",
    "            weights['DATAFO'] += pair_adjustment\n",
    "    \n",
    "    # ----- STRICT LEVERAGE CONTROL -----\n",
    "    # Calculate current leverage\n",
    "    leverage = sum(abs(w) for w in weights.values())\n",
    "    \n",
    "    # If approaching maximum, scale non-cash positions proportionally\n",
    "    if leverage > 2.7:  # Conservative buffer of 0.3\n",
    "        scaling_factor = 2.7 / leverage\n",
    "        for asset in weights:\n",
    "            if asset != 'CASH':\n",
    "                weights[asset] *= scaling_factor\n",
    "    \n",
    "    # Ensure weights sum to 1.0\n",
    "    non_cash_sum = sum(weights[asset] for asset in weights if asset != 'CASH')\n",
    "    weights['CASH'] = 1.0 - non_cash_sum\n",
    "    \n",
    "    # Final integrity check with numerical precision safety\n",
    "    final_leverage = sum(abs(w) for w in weights.values())\n",
    "    if final_leverage >= 2.99:  # If still too close to limit after adjustments\n",
    "        # Emergency fallback to conservative allocation\n",
    "        w_top = 0.5 if sorted_assets[0][1] > 0 else -0.2\n",
    "        w_second = 0.25 if sorted_assets[1][1] > 0 else -0.1\n",
    "        w_cash = 1.0 - (w_top + w_second)\n",
    "        \n",
    "        return np.array([\n",
    "            w_top if top_asset == 'TEC' else (w_second if second_asset == 'TEC' else 0),\n",
    "            w_top if top_asset == 'MKT' else (w_second if second_asset == 'MKT' else 0),\n",
    "            w_top if top_asset == 'DATAFC' else (w_second if second_asset == 'DATAFC' else 0),\n",
    "            w_top if top_asset == 'DATAFO' else (w_second if second_asset == 'DATAFO' else 0),\n",
    "            w_cash\n",
    "        ])\n",
    "    \n",
    "    return np.array([\n",
    "        weights['TEC'],\n",
    "        weights['MKT'],\n",
    "        weights['DATAFC'],\n",
    "        weights['DATAFO'],\n",
    "        weights['CASH']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission & Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The criteria for grading the competition—based on CAGR and Sharpe ratio—naturally extend into two fundamental concepts in finance: portfolio optimization and optimal leverage. These concepts are deeply intertwined with the metrics used and encourage contestants to think about broader principles beyond simple return maximization (which is still important).\n",
    "\n",
    "**Portfolio Optimization (Markowitz Framework)**\n",
    "The use of the Sharpe ratio as a grading metric directly connects to the principles of portfolio optimization, as introduced by Harry Markowitz. In this framework, the goal is to construct a portfolio that maximizes return for a given level of risk or minimizes risk for a given level of return. Contestants are implicitly incentivized to consider not only the predictive accuracy of their strategies but also how their allocations manage risk across the portfolio. For instance, while a highly concentrated allocation might yield a high return, it could expose the portfolio to extreme volatility, reducing the Sharpe ratio. By balancing allocations between assets like SPY, QQQ, DATAFC, and DATAFO—and even the cash component—participants are effectively engaging in a simplified form of Markowitz optimization, seeking to allocate weights in a way that optimally balances risk and reward.\n",
    "\n",
    "**Optimal Leverage (Kelly Criterion)**\n",
    "The inclusion of CAGR as a grading criterion introduces the concept of optimal leverage, as informed by the Kelly Criterion. CAGR measures the long-term growth rate of the portfolio, reflecting the compounding effect of returns over time. To maximize CAGR, contestants must carefully consider the level of leverage in their strategies. While higher leverage can amplify returns, it also increases volatility and the risk of drawdowns, which can severely impact the compounding process. The Kelly Criterion provides a theoretical framework for determining the optimal leverage that maximizes long-term growth while avoiding overexposure to risk. Contestants who understand and apply this principle can find the balance between aggressive and conservative leverage, ensuring their strategies are sustainable over the long run.\n",
    "\n",
    "**Grading for the case** will be based on two key metrics: **compound annual growth rate (CAGR) and Sharpe ratio over one year of out-of-sample (OOS) data.** CAGR evaluates the absolute monetary performance of the strategy, reflecting its ability to maximize returns. The Sharpe ratio measures risk-adjusted returns, highlighting strategies that effectively balance profitability with volatility. While these metrics emphasize different aspects of performance, it is possible for a strategy to excel in all both by achieving high returns with consistent risk management and sustainable growth. Participants are encouraged to design robust strategies that optimize across these dimensions for a comprehensive evaluation.\n",
    "\n",
    "Your final score in the data science case will be determined by the sum of your placements between CAGR rankings and Sharpe ratio rankings. For example, if your team places 7th in CAGR and 2nd in Sharpe Ratio out of the 35 teams, your score will be 9. Lowest score wins.\n",
    "\n",
    "You will submit your trading strategy as a python file. Trying to gain an unfair advantage with outside data will result in immediate disqualification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements a grading function designed to evaluate trading algorithms based on historical data without allowing any \"look-ahead bias.\" Each day, the algorithm is provided with only the data available up to that point, ensuring fairness and adherence to real-world constraints. The function validates the contestant’s allocations against several rules: **allocations must sum to 1.0 and the sum of absolute values of positions cannot exceed 3 (to limit leverage).** It then calculates daily returns based on the provided weights and updates the portfolio's value. The key performance metrics final portfolio value and Sharpe ratio are computed to measure the effectiveness of the strategy. This same function will be applied to one year of out-of-sample (OOS) data to evaluate the robustness of the submitted algorithms under unseen market conditions. Please make sure that your submitted algorithm handles edge cases, such as the presence of only a single row at the beginning of the dataset. **Contestants are strongly encouraged to thoroughly test their code for bugs or violations of the constraints**, as submissions that fail to run correctly or violate the rules will be disqualified to ensure fairness for all participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_strategy(historical_df):\n",
    "    \"\"\"\n",
    "    Dummy strategy:\n",
    "    Returns equal weights (20% each) every day, including 'CASH'.\n",
    "    [w_TEC, w_MKT, w_DATAFC, w_DATAFO, w_CASH]\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    \n",
    "    return w\n",
    "\n",
    "def grade_submission(strategy_func, df, risk_free_rate=0.045):\n",
    "    \"\"\"\n",
    "    Evaluate a strategy by simulating over daily data (no look-ahead),\n",
    "    computing final portfolio value, CAGR, and Sharpe ratio.\n",
    "\n",
    "    Each day, we call `strategy_func(historical_df_up_to_today)`\n",
    "    so that the contestant can produce a separate allocation for that day.\n",
    "\n",
    "    Constraints:\n",
    "    1. Allocations must sum up to 1\n",
    "    2. The sum of absolute values of positions cannot exceed 3.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strategy_func : function\n",
    "        A function that takes a partial (historical) DataFrame:\n",
    "          df.iloc[:i+1], and returns an allocation array/list of 5 floats:\n",
    "          [w_TEC, w_MKT, w_DATAFC, w_DATAFO, w_CASH]\n",
    "\n",
    "    df : pd.DataFrame\n",
    "        Contains daily data sorted by 'Date' with columns:\n",
    "        [\n",
    "          'Date', 'TEC', 'MKT', 'DATAFC', 'DATAFO',\n",
    "          'TEC_VOLUME', 'MKT_VOLUME', 'DATAFC_VOLUME', 'DATAFO_VOLUME',\n",
    "          'MKT_IMBALANCE', 'TEC_IMBALANCE', 'DATAFO_IMBALANCE', 'DATAFC_IMBALANCE'\n",
    "        ]\n",
    "\n",
    "    risk_free_rate : float, optional\n",
    "        Risk-free rate per year (e.g. 0.02 for 2%). Defaults to 0.045 (4.5%).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    final_value : float\n",
    "        The final portfolio value after the last day.\n",
    "\n",
    "    cagr : float\n",
    "        Compound Annual Growth Rate:\n",
    "          final_value^(252 / number_of_trading_days) - 1\n",
    "\n",
    "    annual_sharpe : float\n",
    "        Annualized Sharpe Ratio:\n",
    "          (mean(daily_returns) - rf_daily) / std(daily_returns) * sqrt(252)\n",
    "\n",
    "    allocations_df : pd.DataFrame\n",
    "        A table showing each day's date and the weights used:\n",
    "        [\n",
    "          'Date',\n",
    "          'w_TEC', 'w_MKT', 'w_DATAFC', 'w_DATAFO', 'w_CASH'\n",
    "        ]\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    portfolio_value = 1.0\n",
    "    daily_returns = []\n",
    "    allocations_all_days = []\n",
    "\n",
    "    # Simulate from day 0 to day N-2 (because we look at day i -> i+1 prices)\n",
    "    for i in range(len(df) - 1):\n",
    "        # Provide historical data up to day i (inclusive): NO future data\n",
    "        historical_df = df.iloc[:i+1].copy()\n",
    "\n",
    "        # Contestant's strategy returns today's allocation\n",
    "        w = strategy_func(historical_df)\n",
    "\n",
    "        # Validate it sums to 1.0\n",
    "        if not np.isclose(np.sum(w), 1.0):\n",
    "            raise ValueError(f\"Allocations on day {i} do not sum to 1.0: {w}\")\n",
    "\n",
    "        # Validate leverage\n",
    "        total_leverage = np.sum(np.abs(w))\n",
    "        if total_leverage > 3:\n",
    "            raise ValueError(f\"Leverage exceeded 3 on day {i}: {total_leverage}\")\n",
    "\n",
    "        # Store the allocation along with today's date\n",
    "        allocations_all_days.append([\n",
    "            df.loc[i, 'Date'],\n",
    "            w[0],  # w_TEC\n",
    "            w[1],  # w_MKT\n",
    "            w[2],  # w_DATAFC\n",
    "            w[3],  # w_DATAFO\n",
    "            w[4],  # w_CASH\n",
    "        ])\n",
    "\n",
    "        # Calculate the daily return from day i to day i+1\n",
    "        r_tec    = df.loc[i+1, 'TEC']    / df.loc[i, 'TEC']    - 1\n",
    "        r_mkt    = df.loc[i+1, 'MKT']    / df.loc[i, 'MKT']    - 1\n",
    "        r_datafc = df.loc[i+1, 'DATAFC'] / df.loc[i, 'DATAFC'] - 1\n",
    "        r_datafo = df.loc[i+1, 'DATAFO'] / df.loc[i, 'DATAFO'] - 1\n",
    "\n",
    "        # Weights w = [w_TEC, w_MKT, w_DATAFC, w_DATAFO, w_CASH]\n",
    "        w_tec, w_mkt, w_datafc, w_datafo, w_cash = w\n",
    "        \n",
    "        # Sum up the returns for the day \n",
    "        day_return = (\n",
    "            w_tec    * r_tec +\n",
    "            w_mkt    * r_mkt +\n",
    "            w_datafc * r_datafc +\n",
    "            w_datafo * r_datafo + \n",
    "            w_cash * risk_free_rate / 252\n",
    "        )\n",
    "        \n",
    "        # Update the portfolio value\n",
    "        portfolio_value *= (1 + day_return)\n",
    "\n",
    "        # Check if portfolio value is negative\n",
    "        if portfolio_value < 0:\n",
    "            raise ValueError(f\"Portfolio value became negative on day {i}.\")\n",
    "        \n",
    "        daily_returns.append(day_return)\n",
    "\n",
    "    final_value = portfolio_value\n",
    "    n_days = len(daily_returns)\n",
    "    cagr = final_value**(252 / n_days) - 1\n",
    "\n",
    "    # Compute Sharpe Ratio\n",
    "    rf_daily = risk_free_rate / 252\n",
    "    excess_returns = [r - rf_daily for r in daily_returns]\n",
    "    avg_excess_return = np.mean(excess_returns)\n",
    "    std_excess_return = np.std(excess_returns, ddof=1)\n",
    "    if std_excess_return > 0:\n",
    "        annual_sharpe = (avg_excess_return / std_excess_return) * np.sqrt(252)\n",
    "    else:\n",
    "        annual_sharpe = 0.0\n",
    "\n",
    "    allocations_df = pd.DataFrame(\n",
    "        allocations_all_days,\n",
    "        columns=[\n",
    "            'Date',\n",
    "            'w_TEC',\n",
    "            'w_MKT',\n",
    "            'w_DATAFC',\n",
    "            'w_DATAFO',\n",
    "            'w_CASH'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return final_value, cagr, annual_sharpe, allocations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final portfolio value: 9.050005923834833\n",
      "CAGR:                 202.16%\n",
      "Annual Sharpe ratio:  5.04\n",
      "\n",
      "Daily Allocations:\n",
      "         Date     w_TEC     w_MKT  w_DATAFC  w_DATAFO    w_CASH\n",
      "0  2024-01-03  0.200000  0.200000  0.200000  0.200000  0.200000\n",
      "1  2024-01-04  0.254421  0.600000 -0.050000 -0.005239  0.200818\n",
      "2  2024-01-05 -0.010243 -0.050000  0.526200  0.259436  0.274607\n",
      "3  2024-01-08  0.600000 -0.050000 -0.019414  0.258649  0.210765\n",
      "4  2024-01-09 -0.050000  0.286624  0.600000  0.111457  0.051919\n",
      "5  2024-01-10 -0.008079  0.298554 -0.050000  0.600000  0.159526\n",
      "6  2024-01-11 -0.050000  0.600000  0.261066 -0.033175  0.222109\n",
      "7  2024-01-12 -0.050000 -0.017548  0.256221  0.529723  0.281603\n",
      "8  2024-01-16 -0.001883  0.600000 -0.004099 -0.050000  0.455982\n",
      "9  2024-01-17  0.600000  0.276675  0.000000  0.000000  0.123325\n"
     ]
    }
   ],
   "source": [
    "# Run grading\n",
    "final_val, cagr_val, sharpe_val, allocations_df = grade_submission(optimized_asymmetric_liquidity_strategy, auction_prices)\n",
    "\n",
    "print(\"Final portfolio value:\", final_val)\n",
    "print(\"CAGR:                \", f\"{cagr_val:.2%}\")\n",
    "print(\"Annual Sharpe ratio: \", f\"{sharpe_val:.2f}\")\n",
    "print(\"\\nDaily Allocations:\")\n",
    "print(allocations_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
